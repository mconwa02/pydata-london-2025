{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1: Introduction and Fundamentals\n",
    "\n",
    "#### PyData London 2025 - Bayesian Time Series Analysis with PyMC\n",
    "\n",
    "---\n",
    "\n",
    "The world around us is inherently **dynamic**, with almost everything we care about -- from the environment and economy, to sports and health -- constantly evolving. Time series analysis is the discipline dedicated to making sense of this change by considering data collected sequentially, over time. It aims to uncover underlying patterns such as **trends, seasonal variations, and cyclical movements**, to gain insights into the mechanisms generating the data, and crucially, to forecast future outcomes. The products of time series analysos underpins informed **decision-making**, allows for robust **risk assessment**, and enables strategic **planning based** on projected future behavior.\n",
    "\n",
    "Building upon this, Bayesian time series analysis offers a particularly powerful and adaptable framework. This approach has seen a significant surge in adoption, due to several compelling advantages. It provides a formal structure for integrating **prior information** that might be available before observing the data, a feature that is invaluable when data is sparse. Bayesian methods are also intrinsically suited for sequential learning and **adaptive decision-making**, allowing models to be refined as new information becomes available. Furthermore, Bayesian methods allow for robust inference from **small sample data**, avoiding reliance on asymptotic approximations common in some classical techniques. \n",
    "\n",
    "A key strength of the Bayesian approach is its natural and comprehensive handling of prediction, as it systematically accounts for **parameter**, **model**, and **aleatoric** uncertainty by integrating parameters out with the posterior distribution to create a predictive distribution that is probabilistic. The advent of sophisticated computational tools, most notably Markov chain Monte Carlo (MCMC) methods, has been pivotal, making even complex time series models amenable to practical Bayesian analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RNG = np.random.default_rng(RANDOM_SEED:=42)\n",
    "\n",
    "print(\"üìä Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Characteristics of Time Series Data\n",
    "\n",
    "Before diving into Bayesian methods, let's establish a solid foundation in time series fundamentals. Understanding these characteristics is crucial for building appropriate models and interpreting results effectively.\n",
    "\n",
    "### The Anatomy of Time Series: Core Components\n",
    "\n",
    "Time series data can be understood as a combination of several fundamental components. Think of these as the **\"building blocks\"** that, when combined, create the patterns we observe in real-world data. Understanding each component is crucial for effective modeling and interpretation.\n",
    "\n",
    "#### 1. **Trend ($T_t$)**: The Long-Term Direction\n",
    "\n",
    "The **trend** represents the underlying long-term movement in the data. Trends can take various forms depending on the underlying process generating the data.\n",
    "\n",
    "A **linear trend** shows steady increase or decrease over time, such as population growth or the gradual decline in manufacturing employment in developed countries. **Non-linear trends** exhibit curved patterns, including exponential growth seen in technology adoption or S-shaped curves characteristic of market saturation processes. Some time series exhibit **changing trends** where the direction shifts over time, commonly observed in economic cycles where growth periods alternate with contractions.\n",
    "\n",
    "#### 2. **Seasonality ($S_t$)**: Predictable Recurring Patterns\n",
    "\n",
    "**Seasonal patterns** repeat over fixed, known periods, and their key characteristic is **predictability**‚Äîif you know the season, you can anticipate the pattern with reasonable confidence. This predictability makes seasonality one of the most valuable components for forecasting.\n",
    "\n",
    "**Annual seasonality** appears in phenomena like holiday sales spikes, agricultural production cycles, and weather-dependent energy consumption. **Weekly seasonality** is common in business data, where website traffic peaks on weekdays or retail sales follow consistent weekly patterns. **Daily seasonality** manifests in rush hour traffic patterns, electricity usage that peaks during evening hours, or social media activity that varies throughout the day. Many real-world time series exhibit **multiple seasonality**, such as retail data that shows both weekly patterns (higher weekend sales) and annual patterns (holiday shopping seasons).\n",
    "\n",
    "#### 3. **Cyclical Patterns ($C_t$)**: Irregular Long-Term Fluctuations\n",
    "\n",
    "Unlike seasonality, **cyclical patterns** have variable periods and are often driven by external factors that don't follow a fixed schedule. These cycles represent longer-term fluctuations that can span several years or even decades.\n",
    "\n",
    "**Business cycles** encompass economic expansions and recessions that vary in duration and intensity. **Market cycles** include bull and bear markets in financial data, where periods of growth and decline don't follow predictable timing. **Natural cycles** such as El Ni√±o and La Ni√±a climate patterns affect weather, agriculture, and economic activity over multi-year periods with irregular timing.\n",
    "\n",
    "#### 4. **Irregular/Noise ($\\epsilon_t$)**: The Unpredictable Component\n",
    "\n",
    "The **irregular component** represents random variation that cannot be explained by trend, seasonality, or cycles. This component is inherently unpredictable but understanding its sources helps in model specification and interpretation.\n",
    "\n",
    "**Measurement errors** include sensor noise, rounding errors, and data collection inconsistencies that add random variation to observations. **Random events** such as unexpected news, natural disasters, or policy changes create one-time shocks that don't follow systematic patterns. **Model limitations** also contribute to the irregular component when our models cannot capture patterns that are too complex or when important variables are omitted from the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Dependence\n",
    "\n",
    "The defining characteristic that makes time series data special is **temporal dependence**‚Äîobservations that are close in time are typically more similar than observations that are far apart. This fundamental property violates the assumptions of independence and identical distribution underlying standard statistical methods and necessitates specialized techniques.\n",
    "\n",
    "**Autocorrelation** provides a mathematical framework for quantifying this temporal dependence. The autocorrelation function measures the linear relationship between observations separated by different time intervals:\n",
    "\n",
    "$$\\rho_k = \\frac{\\gamma_k}{\\gamma_0} = \\frac{\\text{Cov}(X_t, X_{t+k})}{\\text{Var}(X_t)}$$\n",
    "\n",
    "The autocorrelation function completely characterizes the linear dependence structure of a stationary time series and is fundamental to time series analysis, particularly in ARMA modeling and spectral analysis.\n",
    "\n",
    "where $h$ represents the **lag** or time separation between observations.\n",
    "\n",
    "Understanding autocorrelation patterns reveals crucial insights about the underlying data generating process. **Strong autocorrelation** at short lags indicates that recent observations are highly predictive of future values, making forecasting possible and effective. Conversely, **weak autocorrelation** suggests that the series behaves more like random noise, making prediction challenging. Different autocorrelation patterns reveal different underlying processes: slowly decaying autocorrelations suggest trending behavior, while oscillating patterns indicate seasonal or cyclical components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "phi = 0.7  \n",
    "sigma = 1.0 \n",
    "n_samples = 500\n",
    "n_lags = 20\n",
    "\n",
    "ar1_data = np.zeros(n_samples)\n",
    "ar1_data[0] = RNG.normal(0, sigma) \n",
    "\n",
    "for t in range(1, n_samples):\n",
    "    ar1_data[t] = phi * ar1_data[t-1] + RNG.normal(0, sigma)\n",
    "\n",
    "autocorr_ar1 = acf(ar1_data, nlags=n_lags, fft=True)\n",
    "\n",
    "n_effective = len(ar1_data)\n",
    "confidence_bound = 1.96 / np.sqrt(n_effective)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\"AR(1) Time Series\", \"Autocorrelation Function\")\n",
    ")\n",
    "\n",
    "time_indices = np.arange(n_samples)\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=time_indices,\n",
    "        y=ar1_data,\n",
    "        mode='lines',\n",
    "        line=dict(color='steelblue', width=1.5),\n",
    "        name='AR(1) Series'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "lags = np.arange(n_lags + 1)\n",
    "for i, (lag, corr) in enumerate(zip(lags, autocorr_ar1)):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[lag, lag],\n",
    "            y=[0, corr],\n",
    "            mode='lines',\n",
    "            line=dict(color='darkred', width=2),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=lags,\n",
    "        y=autocorr_ar1,\n",
    "        mode='markers',\n",
    "        marker=dict(color='darkred', size=8),\n",
    "        name='Empirical'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "theoretical_acf = phi ** lags\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=lags,\n",
    "        y=theoretical_acf,\n",
    "        mode='lines',\n",
    "        line=dict(color='orange', width=2, dash='dash'),\n",
    "        name='Theoretical'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_hline(y=confidence_bound, line=dict(color='gray', width=1, dash='dot'), row=1, col=2)\n",
    "fig.add_hline(y=-confidence_bound, line=dict(color='gray', width=1, dash='dot'), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text='Time', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Value', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Lag', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Autocorrelation', row=1, col=2)\n",
    "\n",
    "fig.update_layout(height=400, legend=dict(orientation='h', y=-0.2))\n",
    "fig.update_layout(\n",
    "    margin=dict(l=50, r=50, t=50, b=50),\n",
    "    height=400, \n",
    "    width=900,\n",
    "    legend=dict(orientation='h', y=-0.2)\n",
    ")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Stationarity\n",
    "\n",
    "A time series is **stationary** if its statistical properties remain constant over time. This concept is fundamental to time series analysis because many statistical methods and theoretical results depend on this assumption.\n",
    "\n",
    "**Strict stationarity** requires that the joint distribution of any collection of observations is invariant to time shifts. However, in practice, we typically work with **weak stationarity** (also called covariance stationarity), which requires three conditions. First, the **mean must be constant**: $E[y_t] = \\mu$ for all time points $t$, meaning the series doesn't exhibit trending behavior. Second, the **variance must be constant**: $\\text{Var}(y_t) = \\sigma^2$ for all $t$, indicating that the variability around the mean doesn't change over time. Third, the **covariance must be time-invariant**: $\\text{Cov}(y_t, y_{t+h})$ depends only on the lag $h$, not on the specific time point $t$.\n",
    "\n",
    "Stationarity matters for several crucial reasons in time series analysis. Many statistical methods, including classical forecasting techniques and some Bayesian models, assume stationarity for their theoretical validity. **Non-stationary series** can lead to spurious relationships where variables appear correlated simply because they both trend over time, even when no true causal relationship exists. Fortunately, many non-stationary series can be made stationary through appropriate **transformations**. Differencing removes trends by computing $y_t - y_{t-1}$, detrending removes systematic time-dependent patterns, and variance-stabilizing transformations like logarithms can address changing variability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 300\n",
    "time_indices = np.arange(n_samples)\n",
    "\n",
    "phi_stationary = 0.2\n",
    "sigma = 1.0\n",
    "\n",
    "stationary_series = RNG.normal(0, sigma / np.sqrt(1 - phi_stationary**2), n_samples)\n",
    "for t in range(1, n_samples):\n",
    "    stationary_series[t] = phi_stationary * stationary_series[t-1] + RNG.normal(0, sigma)\n",
    "\n",
    "phi_nonstationary = 1.0\n",
    "nonstationary_series = np.zeros(n_samples)\n",
    "nonstationary_series[0] = RNG.normal(0, sigma)\n",
    "\n",
    "for t in range(1, n_samples):\n",
    "    nonstationary_series[t] = phi_nonstationary * nonstationary_series[t-1] + RNG.normal(0, sigma)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\n",
    "        'Stationary Series (AR(1), œÜ=0.2)',\n",
    "        'Non-Stationary Series (Random Walk, œÜ=1.0)'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=time_indices, y=stationary_series, mode='lines',\n",
    "               line=dict(color='steelblue', width=1.5), showlegend=False),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=time_indices, y=nonstationary_series, mode='lines',\n",
    "               line=dict(color='red', width=1.5), showlegend=False),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='Time', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Value', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Time', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Value', row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Time Series Components in the Births Dataset\n",
    "\n",
    "Now let's apply our understanding of time series components to a dataset of daily birth counts from the United States. This classic dataset, featured in Gelman et al.'s *Bayesian Data Analysis*, provides an excellent case study for time series modeling because it exhibits multiple overlapping patterns that affect birth rates.\n",
    "\n",
    "<img src=\"images/bda_cover.png\" alt=\"BDA\" width=\"50%\">\n",
    "\n",
    "We'll visualize the data and identify the different components that make up this time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore the births dataset - a classic time series example\n",
    "# Handle null values in the data\n",
    "births_data = pl.read_csv('../data/births.csv', null_values=['null', 'NA', '', 'NULL'])\n",
    "\n",
    "# Filter out rows with null days if any exist\n",
    "births_data = births_data.filter(pl.col('day').is_not_null())\n",
    "\n",
    "# Aggregate to monthly data\n",
    "monthly_births = (births_data\n",
    "    .group_by(['year', 'month'])\n",
    "    .agg(pl.col('births').sum())\n",
    "    .sort(['year', 'month'])\n",
    ")\n",
    "\n",
    "# Create valid dates using the first day of each month\n",
    "monthly_births = monthly_births.with_columns([\n",
    "    pl.date(pl.col('year'), pl.col('month'), 1).alias('date')\n",
    "])\n",
    "\n",
    "# Focus on a 20-year period for clarity (1970-1990)\n",
    "births_subset = (monthly_births\n",
    "    .filter((pl.col('year') >= 1970) & (pl.col('year') <= 1990))\n",
    "    .with_row_index('index')\n",
    ")\n",
    "\n",
    "print(f\"üìà Births Dataset Overview:\")\n",
    "print(f\"   ‚Ä¢ Total months: {births_subset.height}\")\n",
    "print(f\"   ‚Ä¢ Date range: {births_subset['year'].min()} to {births_subset['year'].max()}\")\n",
    "print(f\"   ‚Ä¢ Monthly births range: {births_subset['births'].min():,} to {births_subset['births'].max():,}\")\n",
    "print(f\"   ‚Ä¢ Average monthly births: {births_subset['births'].mean():.0f}\")\n",
    "print(f\"   ‚Ä¢ Standard deviation: {births_subset['births'].std():.0f}\")\n",
    "\n",
    "# Display the first few observations\n",
    "print(f\"\\nüìã First few observations:\")\n",
    "print(births_subset.select(['year', 'month', 'births']).head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = births_subset['date'].to_list()\n",
    "births_values = births_subset['births'].to_numpy()\n",
    "\n",
    "monthly_avg = (births_subset\n",
    "    .group_by('month')\n",
    "    .agg(pl.col('births').mean())\n",
    "    .sort('month')\n",
    "    .select('births')\n",
    "    .to_numpy().flatten()\n",
    ")\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "\n",
    "yearly_data = (births_subset\n",
    "    .group_by('year')\n",
    "    .agg(pl.col('births').mean())\n",
    "    .sort('year')\n",
    ")\n",
    "yearly_years = yearly_data.select('year').to_numpy().flatten()\n",
    "yearly_avg = yearly_data.select('births').to_numpy().flatten()\n",
    "\n",
    "def rolling_window(data, window):\n",
    "    \"\"\"Calculate rolling statistics using numpy\"\"\"\n",
    "    shape = data.shape[:-1] + (data.shape[-1] - window + 1, window)\n",
    "    strides = data.strides + (data.strides[-1],)\n",
    "    rolled = np.lib.stride_tricks.as_strided(data, shape=shape, strides=strides)\n",
    "    return rolled\n",
    "\n",
    "window = 12\n",
    "pad_size = window // 2\n",
    "rolling_mean = np.full(len(births_values), np.nan)\n",
    "rolling_std = np.full(len(births_values), np.nan)\n",
    "\n",
    "for i in range(pad_size, len(births_values) - pad_size):\n",
    "    start_idx = i - pad_size\n",
    "    end_idx = i + pad_size + 1\n",
    "    window_data = births_values[start_idx:end_idx]\n",
    "    rolling_mean[i] = np.mean(window_data)\n",
    "    rolling_std[i] = np.std(window_data)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=(\n",
    "        'üìà Monthly Births (1970-1988)',\n",
    "        'üóìÔ∏è Average Births by Month',\n",
    "        'üìä Average Births by Year',\n",
    "        'üìà Trend and Variability'\n",
    "    ),\n",
    "    specs=[[{}, {}], [{}, {}]]\n",
    ").add_trace(\n",
    "    go.Scatter(\n",
    "        x=dates,\n",
    "        y=births_values,\n",
    "        mode='lines',\n",
    "        line=dict(color='steelblue', width=1.5),\n",
    "        name='Monthly Births',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ").add_trace(\n",
    "    go.Scatter(\n",
    "        x=month_names,\n",
    "        y=monthly_avg,\n",
    "        marker_color='lightcoral',\n",
    "        opacity=0.7,\n",
    "        name='Average Births',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ").add_trace(\n",
    "    go.Scatter(\n",
    "        x=yearly_years,\n",
    "        y=yearly_avg,\n",
    "        mode='lines+markers',\n",
    "        line=dict(color='darkgreen', width=2),\n",
    "        marker=dict(size=6),\n",
    "        name='Yearly Average',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=2, col=1\n",
    ").add_trace(\n",
    "    go.Scatter(\n",
    "        x=dates,\n",
    "        y=births_values,\n",
    "        mode='lines',\n",
    "        line=dict(color='lightblue', width=1),\n",
    "        opacity=0.3,\n",
    "        name='Original',\n",
    "        legendgroup='trend'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ").add_trace(\n",
    "    go.Scatter(\n",
    "        x=dates,\n",
    "        y=rolling_mean,\n",
    "        mode='lines',\n",
    "        line=dict(color='red', width=2),\n",
    "        name='12-Month Trend',\n",
    "        legendgroup='trend'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ").add_trace(\n",
    "    go.Scatter(\n",
    "        x=dates,\n",
    "        y=rolling_mean + rolling_std,\n",
    "        mode='lines',\n",
    "        line=dict(width=0),\n",
    "        showlegend=False,\n",
    "        hoverinfo='skip'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ").add_trace(\n",
    "    go.Scatter(\n",
    "        x=dates,\n",
    "        y=rolling_mean - rolling_std,\n",
    "        mode='lines',\n",
    "        line=dict(width=0),\n",
    "        fill='tonexty',\n",
    "        fillcolor='rgba(255,0,0,0.2)',\n",
    "        name='¬±1 Std Dev',\n",
    "        legendgroup='trend'\n",
    "    ),\n",
    "    row=2, col=2\n",
    ").update_layout(\n",
    "    height=700,\n",
    "    title_text='Time Series Components Analysis',\n",
    "    showlegend=False,\n",
    ")\n",
    "\n",
    "fig.update_yaxes(title_text='Number of Births', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Average Births', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Average Births', row=2, col=1)\n",
    "fig.update_yaxes(title_text='Number of Births', row=2, col=2)\n",
    "fig.update_xaxes(title_text='Year', row=2, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform classical seasonal decomposition using pure numpy. We'll implement a simple moving average for trend estimation (12-month centered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def simple_moving_average(data, window=12):\n",
    "    \"\"\"Calculate centered moving average for trend estimation\"\"\"\n",
    "    trend = np.full_like(data, np.nan, dtype=float)\n",
    "    half_window = window // 2\n",
    "    \n",
    "    for i in range(half_window, len(data) - half_window):\n",
    "        trend[i] = np.mean(data[i - half_window:i + half_window + 1])\n",
    "    \n",
    "    return trend\n",
    "\n",
    "trend_component = simple_moving_average(births_values, window=12)\n",
    "\n",
    "trend_filled = np.where(np.isnan(trend_component), np.nanmean(trend_component), trend_component)\n",
    "detrended = births_values - trend_filled\n",
    "seasonal_component = np.zeros_like(births_values)\n",
    "\n",
    "for month in range(12):\n",
    "    month_indices = np.arange(month, len(births_values), 12)\n",
    "    month_values = detrended[month_indices]\n",
    "    month_values_clean = month_values[~np.isnan(month_values)]\n",
    "    if len(month_values_clean) > 0:\n",
    "        seasonal_effect = np.mean(month_values_clean)\n",
    "        seasonal_component[month_indices] = seasonal_effect\n",
    "\n",
    "residual_component = births_values - trend_filled - seasonal_component\n",
    "\n",
    "class SimpleDecomposition:\n",
    "    def __init__(self, observed, trend, seasonal, resid):\n",
    "        self.observed = observed\n",
    "        self.trend = trend\n",
    "        self.seasonal = seasonal\n",
    "        self.resid = resid\n",
    "\n",
    "decomp_add = SimpleDecomposition(births_values, trend_component, seasonal_component, residual_component)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=4, cols=1,\n",
    "    subplot_titles=(\n",
    "        'üìä Original Data',\n",
    "        'üìà Trend Component',\n",
    "        'üóìÔ∏è Seasonal Component',\n",
    "        'üé≤ Residual Component'\n",
    "    ),\n",
    "    vertical_spacing=0.08\n",
    ")\n",
    "\n",
    "components_add = ['observed', 'trend', 'seasonal', 'resid']\n",
    "colors = ['steelblue', 'darkred', 'darkgreen', 'purple']\n",
    "y_labels = ['Births', 'Trend Level', 'Seasonal Effect', 'Residual']\n",
    "\n",
    "for i, (comp, color, ylabel) in enumerate(zip(components_add, colors, y_labels)):\n",
    "    data = getattr(decomp_add, comp)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=dates,\n",
    "            y=data,\n",
    "            mode='lines',\n",
    "            line=dict(color=color, width=1.5),\n",
    "            name=comp.title(),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=i+1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.update_yaxes(title_text=ylabel, row=i+1, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text='Seasonal Decomposition of Monthly Births Data',\n",
    "    showlegend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation Analysis\n",
    "\n",
    "Let's examine the temporal dependence in our births data by computing and visualizing the autocorrelation function (ACF). This will help us understand how past values influence future values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lags = 36  \n",
    "autocorr = acf(births_values, nlags=max_lags, fft=True)\n",
    "\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=(\n",
    "        'üìä Autocorrelation Function (ACF)',\n",
    "        'üóìÔ∏è Seasonal Autocorrelations'\n",
    "    ),\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "lags = np.arange(max_lags + 1)\n",
    "\n",
    "for i, (lag, corr) in enumerate(zip(lags, autocorr)):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=[lag, lag],\n",
    "            y=[0, corr],\n",
    "            mode='lines',\n",
    "            line=dict(color='steelblue', width=2),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=lags,\n",
    "        y=autocorr,\n",
    "        mode='markers',\n",
    "        marker=dict(color='steelblue', size=6),\n",
    "        name='ACF',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_hline(y=0, line_color='black', line_width=1, opacity=0.3, row=1, col=1)\n",
    "fig.add_hline(y=0.2, line_color='red', line_dash='dash', opacity=0.5, row=1, col=1)\n",
    "fig.add_hline(y=-0.2, line_color='red', line_dash='dash', opacity=0.5, row=1, col=1)\n",
    "\n",
    "seasonal_lags = [12, 24, 36]  # 1, 2, 3 years\n",
    "seasonal_autocorr = [autocorr[lag] for lag in seasonal_lags]\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Bar(\n",
    "        x=['1 Year', '2 Years', '3 Years'],\n",
    "        y=seasonal_autocorr,\n",
    "        marker_color='lightcoral',\n",
    "        opacity=0.7,\n",
    "        name='Seasonal ACF',\n",
    "        showlegend=False\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.add_hline(y=0, line_color='black', line_width=1, opacity=0.3, row=1, col=2)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    title_text='Autocorrelation Analysis',\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='Lag (months)', row=1, col=1)\n",
    "fig.update_yaxes(title_text='Autocorrelation', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Lag (months)', row=1, col=2)\n",
    "fig.update_yaxes(title_text='Autocorrelation', row=1, col=2)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing Techniques\n",
    "\n",
    "Before building Bayesian models, proper data preprocessing is essential for ensuring reliable and interpretable results. This section demonstrates key preprocessing techniques that prepare time series data for effective modeling.\n",
    "\n",
    "### Why Preprocessing Matters\n",
    "\n",
    "Time series preprocessing serves several critical purposes that directly impact the success of Bayesian modeling. **Numerical stability** is perhaps the most important consideration‚Äîstandardization helps MCMC samplers converge more reliably by ensuring that all variables operate on similar scales, preventing numerical overflow or underflow issues that can cause sampling algorithms to fail.\n",
    "\n",
    "**Prior specification** becomes much more intuitive with normalized data. When variables are standardized to have zero mean and unit variance, it's easier to specify reasonable prior distributions since we know the approximate scale of the data. **Model interpretation** is also enhanced because standardized coefficients can be directly compared in terms of their relative importance, and the magnitude of effects becomes more meaningful.\n",
    "\n",
    "Finally, **computational efficiency** improves significantly with well-scaled data. MCMC algorithms explore the parameter space more effectively when the posterior distribution is well-conditioned, leading to faster sampling and better mixing of the chains.\n",
    "\n",
    "### Common Preprocessing Techniques\n",
    "\n",
    "Several preprocessing techniques are commonly used in time series analysis, each serving specific purposes. **Standardization** using the formula $(x - \\mu) / \\sigma$ centers data at zero with unit variance, making it the most popular choice for Bayesian modeling. **Min-Max normalization** scales data to the [0,1] range using $(x - \\min) / (\\max - \\min)$, which is useful when you need bounded variables.\n",
    "\n",
    "**Log transformation** $\\log(x)$ serves multiple purposes: it stabilizes variance in series with changing variability, handles exponential growth patterns, and can make multiplicative relationships additive. **Differencing** computes $x_t - x_{t-1}$ to remove trends and induce stationarity, which is essential for many time series models. Finally, **seasonal decomposition** separates the series into trend, seasonal, and residual components, allowing you to model each component separately or remove seasonal effects before modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_data = births_subset['births'].to_numpy()\n",
    "\n",
    "# 1. Standardization (most common for Bayesian modeling)\n",
    "standardized = (original_data - original_data.mean()) / original_data.std()\n",
    "\n",
    "# 2. Min-Max normalization  \n",
    "min_max_norm = (original_data - original_data.min()) / (original_data.max() - original_data.min())\n",
    "\n",
    "# 3. Log transformation\n",
    "log_transform = np.log(original_data)\n",
    "\n",
    "print(\"üìä Preprocessing Results:\")\n",
    "print(f\"   ‚Ä¢ Original data: mean={original_data.mean():.0f}, std={original_data.std():.0f}\")\n",
    "print(f\"   ‚Ä¢ Standardized: mean={standardized.mean():.3f}, std={standardized.std():.3f}\")\n",
    "print(f\"   ‚Ä¢ Min-Max norm: min={min_max_norm.min():.3f}, max={min_max_norm.max():.3f}\")\n",
    "print(f\"   ‚Ä¢ Log transform: mean={log_transform.mean():.3f}, std={log_transform.std():.3f}\")\n",
    "\n",
    "births_standardized = standardized\n",
    "print(f\"\\n‚úÖ **Selected preprocessing**: Standardized data (mean=0, std=1)\")\n",
    "print(f\"   This choice provides:\")\n",
    "print(f\"   ‚Ä¢ Numerical stability for MCMC sampling\")\n",
    "print(f\"   ‚Ä¢ Easy interpretation of parameters\")\n",
    "print(f\"   ‚Ä¢ Natural scale for prior specification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "Hyndman, R. J., & Athanasopoulos, G. (2021). Forecasting: Principles and Practice (3rd ed.). OTexts. \n",
    "\n",
    "Mills, T. C. (2019). Applied Time Series Analysis: A Practical Guide to Modeling and Forecasting. Academic Press.\n",
    "\n",
    "Nielsen, A. (2019). Practical Time Series Analysis: Prediction with Statistics and Machine Learning. O'Reilly Media."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
