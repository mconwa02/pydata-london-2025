{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4: Advanced Models\n",
    "\n",
    "#### PyData London 2025 - Bayesian Time Series Analysis with PyMC\n",
    "\n",
    "---\n",
    "\n",
    "In this section, we explore advanced techniques for modeling time series data using **generative models** and **Gaussian processes**. These approaches allow us to build flexible, interpretable models that can capture complex patterns in temporal data while quantifying our uncertainty about predictions.\n",
    "\n",
    "A **generative model** is a probabilistic approach where we specify how we believe the data was generated. Rather than simply fitting a curve to observations, we build a model that describes the underlying processes that produce the observed patterns. This philosophy aligns with Bayesian inference, where we combine our prior knowledge about these processes with observed data to update our beliefs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cmap\n",
    "import seaborn as sns\n",
    "import scipy as sp\n",
    "from scipy import signal\n",
    "\n",
    "import pymc as pm\n",
    "import pytensor\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "\n",
    "os.environ['PYTENSOR_FLAGS'] = 'mode=FAST_COMPILE,device=cpu,floatX=float64'\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=SyntaxWarning)\n",
    "\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "RNG = np.random.default_rng(RANDOM_SEED:=42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Generative Births Model\n",
    "\n",
    "Let's demonstrate the power of generative modeling by analyzing the US births dataset.\n",
    "\n",
    "When we think about what influences the number of births on any given day, several factors come to mind. Some births are scheduled (induced labor or caesarean sections), which means fewer births occur on weekends and holidays when hospitals prefer not to schedule elective procedures. There are also natural biological and social patterns that create seasonal variations—for instance, conception rates may vary throughout the year due to social factors, leading to birth rate peaks roughly nine months later.\n",
    "\n",
    "Our **generative approach** means we'll explicitly model each of these effects as separate components that combine to produce the observed birth counts. Rather than using a black-box method that might capture these patterns implicitly, we'll build our understanding of the phenomenon directly into the model structure. This approach offers several advantages:\n",
    "\n",
    "1. **Interpretability**: Each component has a clear real-world interpretation\n",
    "2. **Flexibility**: We can add or modify components based on domain knowledge  \n",
    "3. **Uncertainty quantification**: We get credible intervals for each effect\n",
    "4. **Extrapolation**: The model can make principled predictions beyond the observed data\n",
    "\n",
    "The key patterns we'll model include:\n",
    "- **Day-of-week effects**: The systematic reduction in births on weekends due to scheduling practices\n",
    "- **Holiday effects**: Further reductions on major holidays when medical staff prefer to be off\n",
    "- **Annual seasonality**: Biological and social factors creating within-year patterns\n",
    "- **Long-term trends**: Changes in birth rates due to demographic and social shifts\n",
    "- **Complex periodicities**: Multi-year cycles reflecting economic and societal changes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "births_df = pd.read_csv('../data/births.csv')\n",
    "\n",
    "births_df = births_df.dropna(subset=['day'])\n",
    "births_df = births_df[births_df['day'] <= 31]\n",
    "births_df = births_df[births_df['day'] >= 1]\n",
    "\n",
    "births_df['year'] = births_df['year'].astype(int)\n",
    "births_df['month'] = births_df['month'].astype(int)\n",
    "births_df['day'] = births_df['day'].astype(int)\n",
    "\n",
    "births_df['date_str'] = (births_df['year'].astype(str) + '-' + \n",
    "                         births_df['month'].astype(str).str.zfill(2) + '-' + \n",
    "                         births_df['day'].astype(str).str.zfill(2))\n",
    "births_df['date'] = pd.to_datetime(births_df['date_str'], format='%Y-%m-%d', errors='coerce')\n",
    "\n",
    "births_df = births_df.dropna(subset=['date'])\n",
    "\n",
    "daily_births = births_df.groupby('date')['births'].sum().reset_index()\n",
    "daily_births = daily_births.sort_values('date')\n",
    "\n",
    "y = daily_births['births'].values\n",
    "y_mean = y.mean()\n",
    "y_std = y.std()\n",
    "y_scaled = (y - y_mean) / y_std\n",
    "\n",
    "t_days = (daily_births['date'] - daily_births['date'].min()).dt.days.values\n",
    "t = t_days / t_days.max()  # Scale to [0, 1]\n",
    "years_elapsed = t_days / 365.25\n",
    "\n",
    "print(f\"Total days: {len(y)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data\n",
    "\n",
    "Before building our model, let's examine the birth data to understand its structure and identify the patterns we need to capture. \n",
    "\n",
    "The plot below shows twenty years of daily birth counts in the United States. Even at this scale, several patterns are immediately apparent: the data exhibits considerable day-to-day variation, there appear to be seasonal oscillations, and the overall level seems to change over time. Let's investigate these patterns more systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(daily_births['date'], y, alpha=0.7)\n",
    "plt.title('Daily Birth Counts Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Births')\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This day-of-week pattern immediately reveals the influence of **medical scheduling practices** on birth timing. The relatively flat pattern from Monday through Friday reflects the standard work week, with Tuesday showing a slight peak. The dramatic drop on weekends—particularly Sunday—demonstrates that when given a choice, medical professionals and parents prefer to schedule births during regular working hours. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_births['day_of_week'] = daily_births['date'].dt.day_name()\n",
    "dow_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "avg_by_dow = daily_births.groupby('day_of_week')['births'].mean().reindex(dow_order)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(dow_order, avg_by_dow.values, marker='o', linewidth=2, markersize=8)\n",
    "plt.title('Average Births by Day of Week')\n",
    "plt.ylabel('Average number of births')\n",
    "plt.xlabel('Day of week')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Trend Model\n",
    "\n",
    "Following the principle of starting simple and adding complexity, we begin with the most basic time series model—a **linear trend**. This model assumes that births change at a constant rate over time, which we can express mathematically as:\n",
    "\n",
    "$$y_t = \\alpha + \\beta \\cdot t + \\epsilon_t$$\n",
    "\n",
    "where $y_t$ represents the (scaled) number of births at time $t$, $\\alpha$ is the **intercept** (baseline birth rate), $\\beta$ is the **slope** (rate of change), and $\\epsilon_t$ represents random noise.\n",
    "\n",
    "In Bayesian modeling, we must specify **prior distributions** for our parameters. These priors encode our beliefs about plausible parameter values before seeing the data. For this initial model, we use **weakly informative priors**:\n",
    "\n",
    "- $\\alpha \\sim \\text{Normal}(0, 1)$: Since we've standardized the data, we expect the intercept to be near zero\n",
    "- $\\beta \\sim \\text{Normal}(0, 1)$: We don't expect extreme trends in the standardized data\n",
    "- $\\sigma \\sim \\text{Half-Normal}(1)$: The noise scale should be positive and moderate\n",
    "\n",
    "The beauty of Bayesian modeling is that we can perform a **prior predictive check**—simulating data from our model using only the priors. This helps us verify that our prior choices generate reasonable data patterns before we fit the model to actual observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(check_bounds=False) as linear:\n",
    "\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=1)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=1)\n",
    "    \n",
    "    trend = pm.Deterministic(\"trend\", alpha + beta * t)\n",
    "    \n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n",
    "\n",
    "    pm.Normal(\"likelihood\", mu=trend, sigma=sigma, observed=y_scaled)\n",
    "\n",
    "    prior_pred = pm.sample_prior_predictive(1000, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "prior_samples = prior_pred.prior_predictive[\"likelihood\"].values[0]\n",
    "for i in range(100):\n",
    "    ax.plot(daily_births['date'], prior_samples[i] * y_std + y_mean, alpha=0.01, color=\"blue\")\n",
    "\n",
    "ax.plot(daily_births['date'], y, color=\"red\", linewidth=2, label=\"Actual births\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of births\")\n",
    "ax.set_title(\"Prior predictive check - Linear trend only\")\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's estimate the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with linear:\n",
    "    trace_linear = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED)\n",
    "    post_pred_linear = pm.sample_posterior_predictive(trace_linear, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "post_samples = post_pred_linear.posterior_predictive[\"likelihood\"].values\n",
    "post_mean = post_samples.mean(axis=(0, 1))\n",
    "post_mean_rescaled = post_mean * y_std + y_mean\n",
    "\n",
    "post_samples_flat = post_samples.reshape(-1, post_samples.shape[-1])\n",
    "post_lower = np.percentile(post_samples_flat, 2.5, axis=0) * y_std + y_mean\n",
    "post_upper = np.percentile(post_samples_flat, 97.5, axis=0) * y_std + y_mean\n",
    "\n",
    "ax.plot(daily_births['date'], y, alpha=0.7, label=\"Observed births\")\n",
    "ax.plot(daily_births['date'], post_mean_rescaled, color=\"red\", linewidth=2, label=\"Linear trend\")\n",
    "ax.fill_between(daily_births['date'], post_lower, post_upper, color=\"red\", alpha=0.2, label=\"95% HDI\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of births\")\n",
    "ax.set_title(\"Linear trend model fit\")\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The linear trend model captures the overall increase in births over the 20-year period, but the **wide credible intervals** and clear **systematic patterns** in the observed data tell us we're missing important structure. The oscillating patterns visible in the time series suggest the presence of **seasonal effects**—regular, repeating patterns that occur with a fixed period.\n",
    "\n",
    "Our next step is to extend the model to capture these periodic components."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling Seasonality with Fourier Series\n",
    "\n",
    "To capture periodic patterns, we turn to a powerful mathematical tool: **Fourier series**. The key insight is that any periodic function can be represented as a sum of sine and cosine waves of different frequencies. This decomposition, named after Joseph Fourier, allows us to model complex seasonal patterns using simple building blocks.\n",
    "\n",
    "For a function with period $P$, we can write:\n",
    "\n",
    "$$f(t) = a_0 + \\sum_{n=1}^{N} \\left[ a_n \\cos\\left(\\frac{2\\pi n t}{P}\\right) + b_n \\sin\\left(\\frac{2\\pi n t}{P}\\right) \\right]$$\n",
    "\n",
    "where $N$ is the **order** of the approximation (how many frequency components we include), and $a_n$ and $b_n$ are coefficients that determine the contribution of each frequency.\n",
    "\n",
    "This approach, popularized by Facebook's **Prophet** model, has several advantages for time series modeling:\n",
    "\n",
    "1. **Flexibility**: By including more terms (higher $N$), we can capture increasingly complex patterns\n",
    "2. **Periodicity**: The resulting function is guaranteed to repeat with period $P$\n",
    "3. **Smoothness**: Fourier series naturally produce smooth, continuous functions\n",
    "4. **Orthogonality**: The sine and cosine basis functions are orthogonal, making parameter estimation more stable\n",
    "\n",
    "For birth data, we expect **annual seasonality** with a period of 365.25 days. Let's start by adding this component to our model:\n",
    "\n",
    "$$y_t = \\alpha + \\beta \\cdot t + \\sum_{n=1}^{N} \\left[ a_n \\cos\\left(\\frac{2\\pi n \\cdot \\text{day\\_of\\_year}_t}{365.25}\\right) + b_n \\sin\\left(\\frac{2\\pi n \\cdot \\text{day\\_of\\_year}_t}{365.25}\\right) \\right] + \\epsilon_t$$\n",
    "\n",
    "We'll use $N = 10$ terms initially, which allows the model to capture patterns that repeat up to 10 times per year while maintaining smoothness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_year = daily_births['date'].dt.dayofyear.values\n",
    "n_order_annual = 10  \n",
    "\n",
    "fourier_features_list = []\n",
    "for i in range(1, n_order_annual + 1):\n",
    "    fourier_features_list.append(np.sin(2 * np.pi * i * day_of_year / 365.25))\n",
    "    fourier_features_list.append(np.cos(2 * np.pi * i * day_of_year / 365.25))\n",
    "\n",
    "fourier_features = np.column_stack(fourier_features_list)\n",
    "n_features = fourier_features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fourier features represent periodic oscillations with increasing frequencies. \n",
    "\n",
    "Observe that the sine and cosine features are out-of-phase versions of each other. The key property that makes Fourier features powerful is that any **linear combination** of these features will be periodic. The period of the resulting function is determined by the lowest frequency used in the feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "for i in range(3):\n",
    "    plt.subplot(3, 1, i+1)\n",
    "    plt.plot(daily_births['date'], fourier_features[:, 2*i], color='blue', label=f'Sine (Order {i+1})')\n",
    "    plt.plot(daily_births['date'], fourier_features[:, 2*i+1], color='red', label=f'Cosine (Order {i+1})')\n",
    "    plt.title(f'Fourier Components (Order {i+1})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot above shows the first three pairs of Fourier basis functions. Notice how each successive order oscillates more rapidly: the first-order terms complete one cycle per year, the second-order terms complete two cycles, and so on. The **sine and cosine pairs** are phase-shifted versions of each other—this is crucial because it allows the model to represent a periodic pattern that can peak at any time of year, not just at fixed points determined by our choice of when $t=0$.\n",
    "\n",
    "When we combine these basis functions with learned weights, the model can construct complex seasonal patterns. The Bayesian framework will automatically determine which frequencies are most important by learning appropriate weights from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(daily_births['date'], fourier_features.sum(axis=1))\n",
    "plt.title('Sum of All Fourier Components')\n",
    "plt.grid(True, alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the sum of all Fourier components with equal weights reveals the rich complexity that emerges from combining simple periodic functions. In practice, the model will learn a **weighted combination** of these features, emphasizing some frequencies while diminishing others to best match the observed seasonal pattern in births.\n",
    "\n",
    "The power of this approach becomes clear when we fit the model: the posterior distribution will tell us not just what the seasonal pattern looks like, but also our **uncertainty** about that pattern. Components with strong signals in the data will have tightly constrained coefficients, while unnecessary components will have coefficients centered near zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\"fourier_features\": np.arange(n_features)}\n",
    "\n",
    "with pm.Model(check_bounds=False, coords=coords) as seasonal_model:\n",
    "    \n",
    "    # Priors for trend\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=1)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=1)\n",
    "    trend = pm.Deterministic(\"trend\", alpha + beta * t)\n",
    "\n",
    "    # Priors for seasonality\n",
    "    beta_fourier = pm.Normal(\"beta_fourier\", mu=0, sigma=0.1, dims=\"fourier_features\")\n",
    "    seasonality = pm.Deterministic(\n",
    "        \"seasonality\", pm.math.dot(beta_fourier, fourier_features.T)\n",
    "    )\n",
    "\n",
    "    mu = trend + seasonality\n",
    "    \n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=1)\n",
    "\n",
    "    pm.Normal(\"likelihood\", mu=mu, sigma=sigma, observed=y_scaled)\n",
    "\n",
    "    prior_pred_seasonal = pm.sample_prior_predictive(1000, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prior predictive samples for this model look reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot prior predictive for full model\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot some prior predictions\n",
    "prior_samples_full = prior_pred_seasonal.prior_predictive[\"likelihood\"].values[0]\n",
    "for i in range(100):\n",
    "    ax.plot(daily_births['date'], prior_samples_full[i] * y_std + y_mean, alpha=0.01, color=\"blue\")\n",
    "\n",
    "# Plot actual data\n",
    "ax.plot(daily_births['date'], y, color=\"red\", linewidth=2, label=\"Actual births\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Number of births\")\n",
    "ax.set_title(\"Prior predictive check - Seasonal model (trend + seasonality)\")\n",
    "ax.legend()\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with seasonal_model:\n",
    "\n",
    "    trace_seasonal = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED)\n",
    "\n",
    "    post_pred_seasonal = pm.sample_posterior_predictive(trace_seasonal, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the fitted model\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "# Get posterior predictions\n",
    "post_mean_seasonal = post_pred_seasonal.posterior_predictive[\"likelihood\"].mean(dim=[\"chain\", \"draw\"]).values\n",
    "post_mean_seasonal_rescaled = post_mean_seasonal * y_std + y_mean\n",
    "\n",
    "# Get components\n",
    "seasonality_mean = trace_seasonal.posterior[\"seasonality\"].mean(dim=[\"chain\", \"draw\"]).values\n",
    "\n",
    "# Plot 1: Full model fit\n",
    "ax1.plot(daily_births['date'], y, alpha=0.5, label=\"Observed births\")\n",
    "ax1.plot(daily_births['date'], post_mean_seasonal_rescaled, color=\"red\", linewidth=2, label=\"Model fit\")\n",
    "ax1.set_ylabel(\"Number of births\")\n",
    "ax1.set_title(\"Full model fit: Trend + Seasonality\")\n",
    "ax1.legend()\n",
    "\n",
    "# Plot 2: Seasonality component\n",
    "ax2.plot(daily_births['date'], seasonality_mean, color=\"green\", linewidth=1)\n",
    "ax2.set_xlabel(\"Date\")\n",
    "ax2.set_ylabel(\"Multiplicative effect\")\n",
    "ax2.set_title(\"Seasonality component\")\n",
    "ax2.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "\n",
    "for ax in [ax1, ax2]:\n",
    "    ax.tick_params(axis='x', rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the model is not yet where we want it to be, but we are slowly chipping away at the components of variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The addition of annual seasonality represents a significant improvement over the linear trend model. The seasonal component successfully captures the **within-year variation**, showing higher birth rates in late summer and early fall (roughly September-October) and lower rates in early spring. This pattern aligns with known demographic trends and the \"December conception peak\" that leads to September births.\n",
    "\n",
    "However, careful examination of the model fit reveals we're still missing important structure. The model captures the dominant annual cycle but doesn't account for the **day-of-week patterns** clearly visible in the zoomed views, nor does it handle the dramatic drops in births around **holidays**. Additionally, there may be patterns at other time scales—both shorter (weekly) and longer (multi-year) that could improve our model further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Comprehensive Model\n",
    "\n",
    "Real-world time series often exhibit patterns at multiple time scales simultaneously. Birth data is particularly rich in this regard, influenced by biological, social, medical, and economic factors operating on different frequencies. Let's build a more comprehensive model that captures these various effects.\n",
    "\n",
    "Our enhanced model will include several types of seasonal patterns:\n",
    "\n",
    "**1. Annual Seasonality (Higher Order)**: By increasing from 10 to 15 Fourier terms, we allow the model to capture more nuanced within-year patterns. This helps model phenomena like the \"wedding season\" effect on conception timing or temperature-related conception patterns.\n",
    "\n",
    "**2. Sub-annual (6-month) Seasonality**: Some patterns repeat twice per year. For instance, there might be conception peaks related to major holiday periods (winter holidays and summer vacations) that create a bi-annual birth pattern.\n",
    "\n",
    "**3. Weekly Seasonality**: This captures the strong **medical scheduling effects**. Modern obstetric practice involves many scheduled procedures, leading to fewer births on weekends. We model this with day-of-week Fourier features.\n",
    "\n",
    "**4. Multi-year Cycles**: Demographics and society change on longer timescales. Economic cycles, generational effects, and evolving social norms can create patterns spanning several years.\n",
    "\n",
    "**5. Decadal Cycles**: Even longer patterns emerge from major societal shifts. The post-war baby boom echo, economic recessions, and changing family planning practices create very long-term oscillations. We include periods of 8.5, 10.5, and 11.8 years based on known demographic and economic cycles.\n",
    "\n",
    "**6. Holiday Effects**: Major holidays see dramatic reductions in births due to scheduling. We model these as **indicator variables** that allow the birth rate to drop on specific dates.\n",
    "\n",
    "The full model becomes:\n",
    "\n",
    "$$y_t = \\text{trend}(t) + \\text{annual}(t) + \\text{biannual}(t) + \\text{weekly}(t) + \\text{multiyear}(t) + \\text{decadal}(t) + \\text{holidays}(t) + \\epsilon_t$$\n",
    "\n",
    "where each component is a sum of Fourier terms or indicator functions with learned coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Annual seasonality with more flexibility (higher order)\n",
    "day_of_year = daily_births['date'].dt.dayofyear.values\n",
    "n_order_annual = 15  # Increased from 10 for more flexibility\n",
    "\n",
    "fourier_features_annual = []\n",
    "for i in range(1, n_order_annual + 1):\n",
    "    fourier_features_annual.append(np.sin(2 * np.pi * i * day_of_year / 365.25))\n",
    "    fourier_features_annual.append(np.cos(2 * np.pi * i * day_of_year / 365.25))\n",
    "\n",
    "# 2. Sub-annual (6-month) seasonality\n",
    "fourier_features_biannual = []\n",
    "for i in range(1, 4):  # 3 orders for 6-month periodicity\n",
    "    fourier_features_biannual.append(np.sin(2 * np.pi * i * day_of_year / (365.25/2)))\n",
    "    fourier_features_biannual.append(np.cos(2 * np.pi * i * day_of_year / (365.25/2)))\n",
    "\n",
    "# 3. Weekly seasonality (day-of-week effects)\n",
    "day_of_week = daily_births['date'].dt.dayofweek.values  # 0=Monday, 6=Sunday\n",
    "n_order_weekly = 3\n",
    "\n",
    "fourier_features_weekly = []\n",
    "for i in range(1, n_order_weekly + 1):\n",
    "    fourier_features_weekly.append(np.sin(2 * np.pi * i * day_of_week / 7))\n",
    "    fourier_features_weekly.append(np.cos(2 * np.pi * i * day_of_week / 7))\n",
    "\n",
    "# 4. Multi-year seasonality (medium-term cycles)\n",
    "fourier_features_multiyear = []\n",
    "for period in [2, 3, 4, 5, 7]:  # Years\n",
    "    fourier_features_multiyear.append(np.sin(2 * np.pi * years_elapsed / period))\n",
    "    fourier_features_multiyear.append(np.cos(2 * np.pi * years_elapsed / period))\n",
    "\n",
    "# 5. NEW: Longer-term decadal cycles (8-12 years)\n",
    "# These capture economic cycles, demographic shifts, and other long-term patterns\n",
    "fourier_features_decadal = []\n",
    "for period in [8.5, 10.5, 11.8]:  # Key longer periods\n",
    "    for i in range(1, 2):  # Just 1 order each to keep it efficient\n",
    "        fourier_features_decadal.append(np.sin(2 * np.pi * i * years_elapsed / period))\n",
    "        fourier_features_decadal.append(np.cos(2 * np.pi * i * years_elapsed / period))\n",
    "\n",
    "# 6. Holiday indicators\n",
    "is_new_year = ((daily_births['date'].dt.month == 1) & (daily_births['date'].dt.day == 1)).astype(float)\n",
    "is_christmas = ((daily_births['date'].dt.month == 12) & (daily_births['date'].dt.day == 25)).astype(float)\n",
    "is_christmas_eve = ((daily_births['date'].dt.month == 12) & (daily_births['date'].dt.day == 24)).astype(float)\n",
    "is_july_4th = ((daily_births['date'].dt.month == 7) & (daily_births['date'].dt.day == 4)).astype(float)\n",
    "\n",
    "# Also create indicators for days around major holidays\n",
    "is_near_christmas = ((daily_births['date'].dt.month == 12) & \n",
    "    (daily_births['date'].dt.day >= 23) & \n",
    "    (daily_births['date'].dt.day <= 26)).astype(float)\n",
    "is_near_new_year = (((daily_births['date'].dt.month == 12) & (daily_births['date'].dt.day >= 30)) |\n",
    "    ((daily_births['date'].dt.month == 1) & (daily_births['date'].dt.day <= 2))).astype(float)\n",
    "\n",
    "# Combine all features\n",
    "all_features = (fourier_features_annual + \n",
    "    fourier_features_biannual + \n",
    "    fourier_features_weekly + \n",
    "    fourier_features_multiyear +\n",
    "    fourier_features_decadal)  # Added decadal features\n",
    "\n",
    "fourier_features_all = np.column_stack(all_features)\n",
    "holiday_features = np.column_stack([is_new_year, is_christmas, is_christmas_eve, \n",
    "    is_july_4th, is_near_christmas, is_near_new_year])\n",
    "\n",
    "n_fourier = fourier_features_all.shape[1]\n",
    "n_holidays = holiday_features.shape[1]\n",
    "\n",
    "print(f\"\\nFeature counts:\")\n",
    "print(f\"  Annual Fourier features: {len(fourier_features_annual)}\")\n",
    "print(f\"  Bi-annual Fourier features: {len(fourier_features_biannual)}\")\n",
    "print(f\"  Weekly Fourier features: {len(fourier_features_weekly)}\")\n",
    "print(f\"  Multi-year Fourier features: {len(fourier_features_multiyear)}\")\n",
    "print(f\"  Decadal Fourier features: {len(fourier_features_decadal)}\")\n",
    "print(f\"  Holiday features: {n_holidays}\")\n",
    "print(f\"  Total features: {n_fourier + n_holidays}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the enhanced model with bias adjustment\n",
    "coords_enhanced = {\n",
    "    \"fourier_features\": np.arange(n_fourier),\n",
    "    \"holiday_features\": np.arange(n_holidays),\n",
    "    \"holiday_names\": [\"New Year\", \"Christmas\", \"Christmas Eve\", \"July 4th\", \"Near Christmas\", \"Near New Year\"]\n",
    "}\n",
    "\n",
    "with pm.Model(check_bounds=False, coords=coords_enhanced) as enhanced_model:\n",
    "\n",
    "    # Priors for trend \n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=1)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=1)\n",
    "    \n",
    "    # Add a quadratic term for non-linear trend\n",
    "    beta2 = pm.Normal(\"beta2\", mu=0, sigma=0.5)\n",
    "    trend = pm.Deterministic(\"trend\", alpha + beta * t + beta2 * t**2)\n",
    "\n",
    "    # Priors for Fourier features\n",
    "    beta_fourier = pm.Normal(\"beta_fourier\", mu=0, sigma=0.1, dims=\"fourier_features\")\n",
    "    \n",
    "    # Bookkeeping the Fourier features\n",
    "    idx = 0\n",
    "    beta_annual = beta_fourier[idx:idx+len(fourier_features_annual)]\n",
    "    idx += len(fourier_features_annual)\n",
    "    beta_biannual = beta_fourier[idx:idx+len(fourier_features_biannual)]\n",
    "    idx += len(fourier_features_biannual)\n",
    "    beta_weekly = beta_fourier[idx:idx+len(fourier_features_weekly)]\n",
    "    idx += len(fourier_features_weekly)\n",
    "    beta_multiyear = beta_fourier[idx:idx+len(fourier_features_multiyear)]\n",
    "    idx += len(fourier_features_multiyear)\n",
    "    beta_decadal = beta_fourier[idx:idx+len(fourier_features_decadal)]\n",
    "    \n",
    "    annual_seasonality = pm.math.dot(beta_annual, np.column_stack(fourier_features_annual).T)\n",
    "    biannual_seasonality = pm.math.dot(beta_biannual, np.column_stack(fourier_features_biannual).T)\n",
    "    weekly_seasonality = pm.math.dot(beta_weekly, np.column_stack(fourier_features_weekly).T)\n",
    "    multiyear_seasonality = pm.math.dot(beta_multiyear, np.column_stack(fourier_features_multiyear).T)\n",
    "    decadal_seasonality = pm.math.dot(beta_decadal, np.column_stack(fourier_features_decadal).T)\n",
    "    \n",
    "    beta_holidays = pm.Normal(\"beta_holidays\", mu=-0.3, sigma=0.2, dims=\"holiday_features\")\n",
    "    holiday_effect = pm.math.dot(beta_holidays, holiday_features.T)\n",
    "\n",
    "    total_seasonality = pm.Deterministic(\n",
    "        \"total_seasonality\",\n",
    "        annual_seasonality + \n",
    "        biannual_seasonality + \n",
    "        weekly_seasonality + \n",
    "        multiyear_seasonality + \n",
    "        decadal_seasonality\n",
    "    )\n",
    "    \n",
    "    mu = trend + total_seasonality + holiday_effect \n",
    "\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=0.1)\n",
    "\n",
    "    pm.Normal(\"likelihood\", mu=mu, sigma=sigma, observed=y_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with enhanced_model:\n",
    "    trace_enhanced = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED)\n",
    "\n",
    "    post_pred_enhanced = pm.sample_posterior_predictive(trace_enhanced, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean prediction and rescale to original units\n",
    "post_mean_enhanced = post_pred_enhanced.posterior_predictive[\"likelihood\"].mean(dim=[\"chain\", \"draw\"]).values\n",
    "post_mean_enhanced_rescaled = post_mean_enhanced * y_std + y_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create plots for the enhanced model\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 15))\n",
    "\n",
    "# Plot 1: Enhanced model fit\n",
    "axes[0].plot(daily_births['date'], y, alpha=0.5, label=\"Observed births\", color='blue')\n",
    "axes[0].plot(daily_births['date'], post_mean_enhanced, color=\"green\", linewidth=2, \n",
    "             label=\"Enhanced model\", alpha=0.8)\n",
    "axes[0].set_ylabel(\"Number of births\")\n",
    "axes[0].set_title(\"Enhanced Model Fit: Full Time Series\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot 2: Zoom on 1975 to see weekly patterns\n",
    "zoom_start = pd.to_datetime('1975-01-01')\n",
    "zoom_end = pd.to_datetime('1975-12-31')\n",
    "zoom_mask = (daily_births['date'] >= zoom_start) & (daily_births['date'] <= zoom_end)\n",
    "\n",
    "axes[1].plot(daily_births['date'][zoom_mask], y[zoom_mask], alpha=0.7, label=\"Observed\", \n",
    "             marker='o', markersize=2, color='blue')\n",
    "axes[1].plot(daily_births['date'][zoom_mask], post_mean_enhanced_rescaled[zoom_mask], \n",
    "             color=\"green\", linewidth=2, label=\"Enhanced model\", alpha=0.8)\n",
    "axes[1].set_ylabel(\"Number of births\")\n",
    "axes[1].set_title(\"Enhanced Model Fit - 1975 (showing weekly patterns)\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Enhanced model residuals\n",
    "post_mean_enhanced = post_pred_enhanced.posterior_predictive[\"likelihood\"].mean(dim=[\"chain\", \"draw\"]).values\n",
    "post_mean_enhanced_rescaled = post_mean_enhanced * y_std + y_mean\n",
    "residuals_enhanced = y - post_mean_enhanced_rescaled\n",
    "\n",
    "axes[2].scatter(daily_births['date'], residuals_enhanced, alpha=0.3, s=1, color='green', label='Enhanced model')\n",
    "axes[2].axhline(y=0, color='black', linestyle='--')\n",
    "axes[2].set_ylabel(\"Residuals\")\n",
    "axes[2].set_xlabel(\"Date\")\n",
    "axes[2].set_title(f\"Enhanced Model Residuals (std = {residuals_enhanced.std():.0f})\")\n",
    "axes[2].legend()\n",
    "\n",
    "for ax in axes[:2]:\n",
    "    ax.tick_params(axis='x', rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Holiday Effects\n",
    "\n",
    "The comprehensive model includes specific indicators for major holidays, allowing us to quantify their impact on birth patterns. Let's examine how well the model captures these sharp, localized drops in births around Christmas, New Year's Day, and Independence Day using a couple of arbitrarily-chosen years. These holiday effects are particularly interesting because they represent **discrete shocks** to the system rather than smooth periodic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "# Christmas 1975\n",
    "xmas_start = pd.to_datetime('1975-12-15')\n",
    "xmas_end = pd.to_datetime('1976-01-10')\n",
    "xmas_mask = (daily_births['date'] >= xmas_start) & (daily_births['date'] <= xmas_end)\n",
    "\n",
    "axes[0].plot(daily_births['date'][xmas_mask], y[xmas_mask], 'o-', alpha=0.7, label=\"Observed\")\n",
    "axes[0].plot(daily_births['date'][xmas_mask], post_mean_enhanced_rescaled[xmas_mask], \n",
    "             'r-', linewidth=2, label=\"Model\")\n",
    "axes[0].axvline(pd.to_datetime('1975-12-25'), color='green', linestyle='--', alpha=0.5, label='Christmas')\n",
    "axes[0].axvline(pd.to_datetime('1976-01-01'), color='blue', linestyle='--', alpha=0.5, label='New Year')\n",
    "axes[0].set_ylabel(\"Number of births\")\n",
    "axes[0].set_title(\"Christmas/New Year Period 1975-1976\")\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# July 4th 1976\n",
    "july_start = pd.to_datetime('1976-06-25')\n",
    "july_end = pd.to_datetime('1976-07-15')\n",
    "july_mask = (daily_births['date'] >= july_start) & (daily_births['date'] <= july_end)\n",
    "\n",
    "axes[1].plot(daily_births['date'][july_mask], y[july_mask], 'o-', alpha=0.7, label=\"Observed\")\n",
    "axes[1].plot(daily_births['date'][july_mask], post_mean_enhanced_rescaled[july_mask], \n",
    "             'r-', linewidth=2, label=\"Model\")\n",
    "axes[1].axvline(pd.to_datetime('1976-07-04'), color='red', linestyle='--', alpha=0.5, label='July 4th')\n",
    "axes[1].set_xlabel(\"Date\")\n",
    "axes[1].set_ylabel(\"Number of births\")\n",
    "axes[1].set_title(\"July 4th Period 1976\")\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "for ax in axes:\n",
    "    ax.tick_params(axis='x', rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekly Pattern Analysis\n",
    "\n",
    "One of the most striking features of the birth data is the strong **weekly periodicity**. By extracting the learned coefficients for the weekly Fourier features, we can visualize exactly how the model perceives the day-of-week effect. This decomposition provides a cleaner view than the raw averages we computed earlier, as it controls for other sources of variation like trends and annual seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and visualize the weekly pattern\n",
    "# Create a synthetic week to show the pattern\n",
    "synthetic_week_days = np.arange(7)\n",
    "synthetic_week_features = []\n",
    "\n",
    "for i in range(1, n_order_weekly + 1):\n",
    "    synthetic_week_features.append(np.sin(2 * np.pi * i * synthetic_week_days / 7))\n",
    "    synthetic_week_features.append(np.cos(2 * np.pi * i * synthetic_week_days / 7))\n",
    "\n",
    "# Get the weekly coefficients\n",
    "idx_start = len(fourier_features_annual) + len(fourier_features_biannual)\n",
    "idx_end = idx_start + len(fourier_features_weekly)\n",
    "weekly_coeffs = trace_enhanced.posterior[\"beta_fourier\"].mean(dim=[\"chain\", \"draw\"]).values[idx_start:idx_end]\n",
    "\n",
    "# Calculate the weekly pattern\n",
    "weekly_pattern = np.dot(weekly_coeffs, np.column_stack(synthetic_week_features).T)\n",
    "weekly_pattern_pct = weekly_pattern * 100  # Convert to percentage\n",
    "\n",
    "# Plot the weekly pattern\n",
    "plt.figure(figsize=(10, 5))\n",
    "days = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "plt.bar(days, weekly_pattern_pct)\n",
    "plt.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "plt.ylabel('Effect on births (%)')\n",
    "plt.title('Model-Estimated Weekly Pattern')\n",
    "plt.xticks(rotation=45);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By decomposing the time series into interpretable components, we've gained insights that would be hidden in a black-box approach. The model reveals that weekday births are elevated by approximately 25-70% compared to the baseline, while Sunday births drop by over 80%. This dramatic pattern reflects the reality of modern obstetric practice, where scheduled procedures dominate weekday deliveries. Similarly, the holiday effects show even more extreme reductions, with some holidays seeing birth rates fall to less than 70% of normal levels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Forecasting Model with pm.Data\n",
    "\n",
    "In the previous section, we saw how to use a \"dummy model\" to generate forecasts. Another approach is to use `pm.Data` containers for all input data. This allows us to easily update the data for forecasting without redefining the model.\n",
    "\n",
    "To demonstrate, let's generate a forecast from our generative model.\n",
    "\n",
    "Start by splitting off a test set from the end of the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = int(0.8 * len(y_scaled))\n",
    "train_idx = np.arange(n_train)\n",
    "test_idx = np.arange(n_train, len(y_scaled))\n",
    "\n",
    "t_train, t_test = t[train_idx], t[test_idx]\n",
    "y_scaled_train, y_scaled_test = y_scaled[train_idx], y_scaled[test_idx]\n",
    "\n",
    "fourier_features_annual_train = [f[train_idx] for f in fourier_features_annual]\n",
    "fourier_features_annual_test = [f[test_idx] for f in fourier_features_annual]\n",
    "\n",
    "fourier_features_biannual_train = [f[train_idx] for f in fourier_features_biannual]\n",
    "fourier_features_biannual_test = [f[test_idx] for f in fourier_features_biannual]\n",
    "\n",
    "fourier_features_weekly_train = [f[train_idx] for f in fourier_features_weekly]\n",
    "fourier_features_weekly_test = [f[test_idx] for f in fourier_features_weekly]\n",
    "\n",
    "fourier_features_multiyear_train = [f[train_idx] for f in fourier_features_multiyear]\n",
    "fourier_features_multiyear_test = [f[test_idx] for f in fourier_features_multiyear]\n",
    "\n",
    "fourier_features_decadal_train = [f[train_idx] for f in fourier_features_decadal]\n",
    "fourier_features_decadal_test = [f[test_idx] for f in fourier_features_decadal]\n",
    "\n",
    "holiday_features_train = holiday_features[train_idx]\n",
    "holiday_features_test = holiday_features[test_idx]\n",
    "\n",
    "fourier_features_all_train = np.column_stack([f for features in \n",
    "    [fourier_features_annual_train, fourier_features_biannual_train, \n",
    "     fourier_features_weekly_train, fourier_features_multiyear_train,\n",
    "     fourier_features_decadal_train] for f in features])\n",
    "fourier_features_all_test = np.column_stack([f for features in \n",
    "    [fourier_features_annual_test, fourier_features_biannual_test, \n",
    "     fourier_features_weekly_test, fourier_features_multiyear_test,\n",
    "     fourier_features_decadal_test] for f in features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we wrap all of the input data in `pm.Data` containers. This essentially adds the data as nodes in the PyMC graph, and allows us to update the model with new data later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(check_bounds=False, coords=coords_enhanced) as enhanced_model:\n",
    "    \n",
    "    t_data = pm.Data(\"t_data\", t_train)\n",
    "    y_scaled_data = pm.Data(\"y_scaled_data\", y_scaled_train)\n",
    "    fourier_annual_data = pm.Data(\"fourier_annual_data\", np.column_stack(fourier_features_annual_train))\n",
    "    fourier_biannual_data = pm.Data(\"fourier_biannual_data\", np.column_stack(fourier_features_biannual_train))\n",
    "    fourier_weekly_data = pm.Data(\"fourier_weekly_data\", np.column_stack(fourier_features_weekly_train))\n",
    "    fourier_multiyear_data = pm.Data(\"fourier_multiyear_data\", np.column_stack(fourier_features_multiyear_train))\n",
    "    fourier_decadal_data = pm.Data(\"fourier_decadal_data\", np.column_stack(fourier_features_decadal_train))\n",
    "    holiday_data = pm.Data(\"holiday_data\", holiday_features_train)\n",
    "\n",
    "    alpha = pm.Normal(\"alpha\", mu=0, sigma=1)\n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=1)\n",
    "    \n",
    "    beta2 = pm.Normal(\"beta2\", mu=0, sigma=0.5)\n",
    "    trend = pm.Deterministic(\"trend\", alpha + beta * t_data + beta2 * t_data**2)\n",
    "\n",
    "    beta_fourier = pm.Normal(\"beta_fourier\", mu=0, sigma=0.1, dims=\"fourier_features\")\n",
    "    \n",
    "    idx = 0\n",
    "    beta_annual = beta_fourier[idx:idx+fourier_annual_data.shape[1]]\n",
    "    idx += fourier_annual_data.shape[1]\n",
    "    beta_biannual = beta_fourier[idx:idx+fourier_biannual_data.shape[1]]\n",
    "    idx += fourier_biannual_data.shape[1]\n",
    "    beta_weekly = beta_fourier[idx:idx+fourier_weekly_data.shape[1]]\n",
    "    idx += fourier_weekly_data.shape[1]\n",
    "    beta_multiyear = beta_fourier[idx:idx+fourier_multiyear_data.shape[1]]\n",
    "    idx += fourier_multiyear_data.shape[1]\n",
    "    beta_decadal = beta_fourier[idx:idx+fourier_decadal_data.shape[1]]\n",
    "    \n",
    "    annual_seasonality = pm.math.dot(beta_annual, fourier_annual_data.T)\n",
    "    biannual_seasonality = pm.math.dot(beta_biannual, fourier_biannual_data.T)\n",
    "    weekly_seasonality = pm.math.dot(beta_weekly, fourier_weekly_data.T)\n",
    "    multiyear_seasonality = pm.math.dot(beta_multiyear, fourier_multiyear_data.T)\n",
    "    decadal_seasonality = pm.math.dot(beta_decadal, fourier_decadal_data.T)\n",
    "    \n",
    "    beta_holidays = pm.Normal(\"beta_holidays\", mu=-0.3, sigma=0.2, dims=\"holiday_features\")\n",
    "    holiday_effect = pm.math.dot(beta_holidays, holiday_data.T)\n",
    "\n",
    "    total_seasonality = pm.Deterministic(\n",
    "        \"total_seasonality\",\n",
    "        annual_seasonality + \n",
    "        biannual_seasonality + \n",
    "        weekly_seasonality + \n",
    "        multiyear_seasonality + \n",
    "        decadal_seasonality\n",
    "    )\n",
    "    \n",
    "    mu = trend + total_seasonality + holiday_effect \n",
    "\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=0.1)\n",
    "\n",
    "    pm.Normal(\"likelihood\", mu=mu, sigma=sigma, observed=y_scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with enhanced_model:\n",
    "    trace_enhanced = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having done all of that, generating forecasts is a matter of swapping in the test data using `pm.set_data` and then sampling from the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with enhanced_model:\n",
    "\n",
    "    # Update all data containers with test data\n",
    "    pm.set_data({\n",
    "        \"t_data\": t_test,\n",
    "        # Dummy values since we're predicting these\n",
    "        \"y_scaled_data\": np.zeros_like(y_scaled_test),  \n",
    "        \"fourier_annual_data\": np.column_stack(fourier_features_annual_test),\n",
    "        \"fourier_biannual_data\": np.column_stack(fourier_features_biannual_test),\n",
    "        \"fourier_weekly_data\": np.column_stack(fourier_features_weekly_test),\n",
    "        \"fourier_multiyear_data\": np.column_stack(fourier_features_multiyear_test),\n",
    "        \"fourier_decadal_data\": np.column_stack(fourier_features_decadal_test),\n",
    "        \"holiday_data\": holiday_features_test\n",
    "    })\n",
    "    \n",
    "    # Generate posterior predictive samples for the test period\n",
    "    forecast_samples = pm.sample_posterior_predictive(\n",
    "        trace_enhanced,\n",
    "        var_names=[\"likelihood\"],\n",
    "        random_seed=RANDOM_SEED\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_samples_array = forecast_samples.posterior_predictive[\"likelihood\"].values\n",
    "forecast_samples_flat = forecast_samples_array.reshape(-1, len(y_scaled_test))\n",
    "\n",
    "forecast_mean = np.mean(forecast_samples_flat, axis=0)\n",
    "forecast_median = np.median(forecast_samples_flat, axis=0)\n",
    "forecast_hdi_90_lower = np.percentile(forecast_samples_flat, 5, axis=0)\n",
    "forecast_hdi_90_upper = np.percentile(forecast_samples_flat, 95, axis=0)\n",
    "forecast_hdi_50_lower = np.percentile(forecast_samples_flat, 25, axis=0)\n",
    "forecast_hdi_50_upper = np.percentile(forecast_samples_flat, 75, axis=0)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 8))\n",
    "\n",
    "time_all = np.arange(len(y_scaled))\n",
    "time_train = time_all[:n_train]\n",
    "time_test = time_all[n_train:]\n",
    "\n",
    "ax.plot(time_train, y_scaled_train, 'o-', color='blue', alpha=0.3, \n",
    "        label='Training Data', markersize=2, linewidth=0.5)\n",
    "\n",
    "ax.plot(time_test, y_scaled_test, 'o-', color='red', alpha=0.5, \n",
    "        label='Testing Data', markersize=3, linewidth=1)\n",
    "\n",
    "light_green = '#4CAF50'\n",
    "\n",
    "ax.plot(time_test, forecast_mean, '-', color=light_green, linewidth=2, \n",
    "        label='Forecast Mean')\n",
    "\n",
    "ax.fill_between(time_test, \n",
    "                forecast_hdi_90_lower, \n",
    "                forecast_hdi_90_upper, \n",
    "                alpha=0.15, color=light_green, label='90% HDI')\n",
    "\n",
    "ax.fill_between(time_test, \n",
    "                forecast_hdi_50_lower, \n",
    "                forecast_hdi_50_upper, \n",
    "                alpha=0.3, color=light_green, label='50% HDI')\n",
    "\n",
    "ax.axvline(n_train, color='black', linestyle='--', alpha=0.7, label='Train/Test Split')\n",
    "\n",
    "ax.set_title('Bayesian Time Series Forecasting with Uncertainty', fontsize=14)\n",
    "ax.set_xlabel('Time', fontsize=12)\n",
    "ax.set_ylabel('Standardized Births', fontsize=12)\n",
    "\n",
    "ax.legend(loc='best', fontsize=9)\n",
    "\n",
    "ax.grid(True, alpha=0.15)\n",
    "\n",
    "ax.set_facecolor('#f8f8f8')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "forecast_errors_standardized = y_scaled_test - forecast_mean\n",
    "ax.plot(time_test, forecast_errors_standardized, 'o-', color='purple', \n",
    "        alpha=0.5, markersize=3, linewidth=1)\n",
    "ax.axhline(0, color='black', linestyle='-', alpha=0.5)\n",
    "ax.set_title('Forecast Errors', fontsize=14)\n",
    "ax.set_xlabel('Time', fontsize=12)\n",
    "ax.set_ylabel('Error (Actual - Forecast)', fontsize=12)\n",
    "ax.grid(True, alpha=0.15)\n",
    "ax.set_facecolor('#f8f8f8')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non-parametric Bayesian Models\n",
    "\n",
    "Use of the term \"non-parametric\" in the context of Bayesian analysis is something of a misnomer. This is because the first and fundamental step in Bayesian modeling is to specify a *full probability model* for the problem at hand. It is rather difficult to explicitly state a full probability model without the use of probability functions, which are parametric. Bayesian non-parametric methods do not imply that there are no parameters, but rather that the number of parameters grows with the size of the dataset. In fact, Bayesian non-parametric models are *infinitely* parametric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building models with Gaussians\n",
    "\n",
    "What if we chose to use Gaussian distributions to model our data? \n",
    "\n",
    "$$p(x \\mid \\pi, \\Sigma) = (2\\pi)^{-k/2}|\\Sigma|^{-1/2} \\exp\\left\\{ -\\frac{1}{2} (x-\\mu)^{\\prime}\\Sigma^{-1}(x-\\mu) \\right\\}$$\n",
    "\n",
    "There would not seem to be an advantage to doing this, because normal distributions are not particularly flexible distributions in and of themselves. However, adopting a set of Gaussians (a multivariate normal vector) confers a number of advantages. First, the marginal distribution of any subset of elements from  a multivariate normal distribution is also normal:\n",
    "\n",
    "$$p(x,y) = \\mathcal{N}\\left(\\left[{\n",
    "\\begin{array}{c}\n",
    "  {\\mu_x}  \\\\\n",
    "  {\\mu_y}  \\\\\n",
    "\\end{array}\n",
    "}\\right], \\left[{\n",
    "\\begin{array}{c}\n",
    "  {\\Sigma_x} & {\\Sigma_{xy}}  \\\\\n",
    "  {\\Sigma_{xy}^T} & {\\Sigma_y}  \\\\\n",
    "\\end{array}\n",
    "}\\right]\\right)$$\n",
    "\n",
    "$$p(x) = \\int p(x,y) dy = \\mathcal{N}(\\mu_x, \\Sigma_x)$$\n",
    "\n",
    "Also, conditionals distributions of a subset of a multivariate normal distribution (conditional on the remaining elements) are normal too:\n",
    "\n",
    "$$p(x|y) = \\mathcal{N}(\\mu_x + \\Sigma_{xy}\\Sigma_y^{-1}(y-\\mu_y), \n",
    "\\Sigma_x-\\Sigma_{xy}\\Sigma_y^{-1}\\Sigma_{xy}^T)$$\n",
    "\n",
    "A Gaussian process generalizes the multivariate normal to infinite dimension. It is defined as an infinite collection of random variables, any finite subset of which have a Gaussian distribution. Thus, the marginalization property is explicit in its definition. Another way of thinking about an infinite vector is as a *function*. When we write a function that takes continuous values as inputs, we are essentially specifying an infinte vector that only returns values (indexed by the inputs) when the function is called upon to do so. By the same token, this notion of an infinite-dimensional Gaussian as a function allows us to work with them computationally: we are never required to store all the elements of the Gaussian process, only to calculate them on demand.\n",
    "\n",
    "So, we can describe a Gaussian process as a ***disribution over functions***. Just as a multivariate normal distribution is completely specified by a mean vector and covariance matrix, a GP is fully specified by a **mean function** and a **covariance function**:\n",
    "\n",
    "$$p(x) \\sim \\mathcal{GP}(m(x), k(x,x^{\\prime}))$$\n",
    "\n",
    "It is the marginalization property that makes working with a Gaussian process feasible: we can marginalize over the infinitely-many variables that we are not interested in, or have not observed. \n",
    "\n",
    "For example, one specification of a GP might be as follows:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m(x) &=0 \\\\\n",
    "k(x,x^{\\prime}) &= \\theta_1\\exp\\left(-\\frac{\\theta_2}{2}(x-x^{\\prime})^2\\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "here, the covariance function is a **squared exponential**, for which values of $x$ and $x^{\\prime}$ that are close together result in values of $k$ closer to 1 and those that are far apart return values closer to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponential_cov(x, y, scale, length_scale):\n",
    "    return scale * np.exp( -0.5 * length_scale * np.subtract.outer(x, y)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12,5))\n",
    "xrange = np.linspace(0, 5)\n",
    "ax1.plot(xrange, exponential_cov(0, xrange, 1, 1))\n",
    "ax1.set_xlabel('x')\n",
    "ax1.set_ylabel('cov(0, x)')\n",
    "\n",
    "z = np.array([exponential_cov(xrange, xprime, 1, 1) for xprime in xrange])\n",
    "ax2.imshow(z, cmap=\"inferno\", \n",
    "       interpolation='none', \n",
    "       extent=(0, 5, 5, 0))\n",
    "ax2.set_xlabel('x')\n",
    "ax2.set_ylabel('x')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may seem odd to simply adopt the zero function to represent the mean function of the Gaussian process -- surely we can do better than that! It turns out that most of the learning in the GP involves the covariance function and its parameters, so very little is gained in specifying a complicated mean function.\n",
    "\n",
    "For a finite number of points, the GP becomes a multivariate normal, with the mean and covariance as the mean functon and covariance function evaluated at those points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian processes regression\n",
    "\n",
    "The following simulated data clearly shows some type of non-linear process, corrupted by a certain amount of observation or measurement error so it should be a reasonable task for a Gaussian process approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the seed\n",
    "np.random.seed(1)\n",
    "\n",
    "n_sim = 100 # The number of data points\n",
    "x_sim = np.linspace(0, 10, n_sim)\n",
    "X_sim = x_sim[:, None] # The inputs to the GP, they must be arranged as a column vector\n",
    "\n",
    "# Define the true covariance function and its parameters\n",
    "l_true = 1.0\n",
    "eta_true = 3.0\n",
    "cov_func = eta_true**2 * pm.gp.cov.Matern52(1, l_true)\n",
    "\n",
    "# A mean function that is zero everywhere\n",
    "mean_func = pm.gp.mean.Zero()\n",
    "\n",
    "# The latent function values are one sample from a multivariate normal\n",
    "# Note that we have to call `eval()` because PyMC built on top of PyTensor\n",
    "f_true = np.random.multivariate_normal(mean_func(X_sim).eval(), \n",
    "    cov_func(X_sim).eval() + 1e-8*np.eye(n_sim), 1).flatten()\n",
    "\n",
    "# The observed data is the latent function plus a small amount of IID Gaussian noise\n",
    "# The standard deviation of the noise is `sigma`\n",
    "sigma_true = 2.0\n",
    "y = f_true + sigma_true * np.random.randn(n_sim)\n",
    "\n",
    "## Plot the data and the unobserved latent function\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "ax.plot(X_sim, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "ax.plot(X_sim, y, 'ok', ms=3, alpha=0.5, label=\"Data\");\n",
    "ax.set_xlabel(\"X\"); ax.set_ylabel(\"y\"); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marginal Likelihood Implementation\n",
    "\n",
    "The `gp.Marginal` class in PyMC implements the simplest case of GP regression:  the observed data are the sum of a GP and Gaussian noise.  `gp.Marginal` has a `marginal_likelihood` method, a `conditional` method, and a `predict` method.  Given a mean and covariance function, the function $f(x)$ is modeled as,\n",
    "\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}(m(x),\\, k(x, x')) \\,.\n",
    "$$\n",
    "\n",
    "The observations $y$ are the unknown function plus noise\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\epsilon &\\sim N(0, \\Sigma) \\\\\n",
    "  y &= f(x) + \\epsilon \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### The Marginal Likelihood\n",
    "\n",
    "The marginal likelihood is the normalizing constant for the posterior distribution, and is the integral of the product of the likelihood and prior.\n",
    "\n",
    "$$p(y|X) = \\int_f p(y|f,X)p(f|X) df$$\n",
    "\n",
    "where for Gaussian processes, we are marginalizing over function values $f$ (instead of parameters $\\theta$).\n",
    "\n",
    "**GP prior**:\n",
    "\n",
    "$$\\log p(f|X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|K| -\\frac{1}{2}f^TK^{-1}f $$\n",
    "\n",
    "**Gaussian likelihood**:\n",
    "\n",
    "$$\\log p(y|f,X) = - \\frac{k}{2}\\log2\\pi - \\frac{1}{2}\\log|\\sigma^2I| -\\frac{1}{2}(y-f)^T(\\sigma^2I)^{-1}(y-f) $$\n",
    "\n",
    "**Marginal likelihood**:\n",
    "\n",
    "$$\\log p(y|X) = \\underbrace{- \\frac{k}{2}\\log2\\pi}_{\\text{constant}} \\underbrace{- \\frac{1}{2}\\log|K + \\sigma^2I|}_{\\text{parameter penalty}} \\underbrace{- \\frac{1}{2}y^T(K+\\sigma^2I)^{-1}y}_{\\text{data fit}} $$\n",
    "\n",
    "Notice that the marginal likelihood includes both a data fit term $- \\frac{1}{2}y^T(K+\\sigma^2I)^{-1}y$ and a parameter penalty term $\\frac{1}{2}\\log|K + \\sigma^2I|$. Hence, the marginal likelihood can help us select an appropriate covariance function, based on its fit to the dataset at hand.\n",
    "\n",
    "### Choosing parameters\n",
    "\n",
    "This is relevant because we have to make choices regarding the parameters of our Gaussian process; they were chosen arbitrarily for the random functions we demonstrated above.\n",
    "\n",
    "For example, in the squared exponential covariance function, we must choose two parameters:\n",
    "\n",
    "$$k(x,x^{\\prime}) = \\theta_1\\exp\\left(-\\frac{\\theta_2}{2}(x-x^{\\prime})^2\\right)$$\n",
    "\n",
    "The first parameter $\\theta_1$ is a scale parameter, which allows the function to yield values outside of the unit interval. The second parameter $\\theta_2$ is a length scale parameter that determines the degree of covariance between $x$ and $x^{\\prime}$; smaller values will tend to smooth the function relative to larger values.\n",
    "\n",
    "We can use the **marginal likelihood** to select appropriate values for these parameters, since it trades off model fit with model complexity. Thus, an optimization procedure can be used to select values for $\\theta$ that maximize the marginial likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance functions\n",
    "\n",
    "The behavior of individual realizations from the GP is governed by the covariance function. This function controls both the degree of *shrinkage* to the mean function and the *smoothness* of functions sampled from the GP.\n",
    "\n",
    "PyMC includes a library of covariance functions to choose from. A flexible choice to start with is the Mat&#232;rn covariance. \n",
    "\n",
    "$$k_{M}(x) = \\frac{\\sigma^2}{\\Gamma(\\nu)2^{\\nu-1}} \\left(\\frac{\\sqrt{2 \\nu} x}{l}\\right)^{\\nu} K_{\\nu}\\left(\\frac{\\sqrt{2 \\nu} x}{l}\\right)$$\n",
    "\n",
    "where where $\\Gamma$ is the gamma function and $K$ is a modified Bessel function. The form of covariance matrices sampled from this function is governed by three parameters, each of which controls a property of the covariance.\n",
    "\n",
    "* **amplitude** ($\\sigma$) controls the scaling of the output along the y-axis. This parameter is just a scalar multiplier, and is therefore usually left out of implementations of the Mat&#232;rn function (*i.e.* set to one)\n",
    "\n",
    "* **lengthscale** ($l$) complements the amplitude by scaling realizations on the x-axis. Larger values make points appear closer together.\n",
    "\n",
    "* **roughness** ($\\nu$) controls the sharpness of ridges in the covariance function, which ultimately affect the roughness (smoothness) of realizations.\n",
    "\n",
    "Though in general all the parameters are non-negative real-valued, when $\\nu = p + 1/2$ for integer-valued $p$, the function can be expressed partly as a polynomial function of order $p$ and generates realizations that are $p$-times differentiable, so values $\\nu \\in \\{3/2, 5/2\\}$ are extremely common."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To provide an idea regarding the variety of forms or covariance functions, here's small selection of available ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid = np.linspace(0,2,200)[:,None]\n",
    "\n",
    "# function to display covariance matrices\n",
    "def plot_cov(X, K, stationary=True):\n",
    "    K = K + 1e-8*np.eye(X.shape[0])\n",
    "    x = X.flatten()\n",
    "    \n",
    "    with sns.axes_style(\"white\"):\n",
    "\n",
    "        fig = plt.figure(figsize=(14,5))\n",
    "        ax1 = fig.add_subplot(121)\n",
    "        m = ax1.imshow(K, cmap=\"inferno\", \n",
    "                       interpolation='none', \n",
    "                       extent=(np.min(X), np.max(X), np.max(X), np.min(X))); \n",
    "        plt.colorbar(m);\n",
    "        ax1.set_title(\"Covariance Matrix\")\n",
    "        ax1.set_xlabel(\"X\")\n",
    "        ax1.set_ylabel(\"X\")\n",
    "\n",
    "        ax2 = fig.add_subplot(122)\n",
    "        if not stationary:\n",
    "            ax2.plot(x, np.diag(K), \"k\", lw=2, alpha=0.8)\n",
    "            ax2.set_title(\"The Diagonal of K\")\n",
    "            ax2.set_ylabel(\"k(x,x)\")\n",
    "        else:\n",
    "            ax2.plot(x, K[:,0], \"k\", lw=2, alpha=0.8)\n",
    "            ax2.set_title(\"K as a function of x - x'\")\n",
    "            ax2.set_ylabel(\"k(x,x')\")\n",
    "        ax2.set_xlabel(\"X\")\n",
    "\n",
    "        fig = plt.figure(figsize=(14,4))\n",
    "        ax = fig.add_subplot(111)\n",
    "        samples = np.random.multivariate_normal(np.zeros(200), K, 5).T;\n",
    "        for i in range(samples.shape[1]):\n",
    "            ax.plot(x, samples[:,i], color=cmap.inferno(i*0.2), lw=2);\n",
    "        ax.set_title(\"Samples from GP Prior\")\n",
    "        ax.set_xlabel(\"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quadratic exponential covariance\n",
    "\n",
    "This is the squared exponential covariance function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    l = 0.2\n",
    "    tau = 2.0\n",
    "    b = 0.5\n",
    "    cov = b + tau * pm.gp.cov.ExpQuad(1, l)\n",
    "\n",
    "K = pytensor.function([], cov(X_grid))()\n",
    "\n",
    "plot_cov(X_grid, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matern $\\nu=3/2$ covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    l = 0.2\n",
    "    tau = 2.0\n",
    "    cov = tau * pm.gp.cov.Matern32(1, l)\n",
    "\n",
    "K = pytensor.function([], cov(X_grid))()\n",
    "\n",
    "plot_cov(X_grid, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine covariance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    l = 0.2\n",
    "    tau = 2.0\n",
    "    cov = tau * pm.gp.cov.Cosine(1, l)\n",
    "\n",
    "K = pytensor.function([], cov(X_grid))()\n",
    "\n",
    "plot_cov(X_grid, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a general idea about covariance functions, let's begin by defining one for our first model.\n",
    "\n",
    "We can use a Matern(5/2) covariance to model our simulated data, and pass this as the `cov_func` argument to the `Marginal` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    \n",
    "    l = pm.InverseGamma(\"l\", alpha=2, beta=1)\n",
    "    eta = pm.HalfCauchy(\"eta\", beta=5)\n",
    "    \n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, l)\n",
    "    mean = pm.gp.mean.Constant(c=1)\n",
    "    gp = pm.gp.Marginal(mean_func=mean, cov_func=cov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `.marginal_likelihood` method\n",
    "\n",
    "The unknown latent function can be analytically integrated out of the product of the GP prior probability with a normal likelihood.  This quantity is called the marginal likelihood. \n",
    "\n",
    "$$\n",
    "p(y \\mid x) = \\int p(y \\mid f, x) \\, p(f \\mid x) \\, df\n",
    "$$\n",
    "\n",
    "The log of the marginal likelihood, $p(y \\mid x)$, is\n",
    "\n",
    "$$\n",
    "\\log p(y \\mid x) = \n",
    "  -\\frac{1}{2} (\\mathbf{y} - \\mathbf{m}_x)^{T} \n",
    "               (\\mathbf{K}_{xx} + \\boldsymbol\\Sigma)^{-1} \n",
    "               (\\mathbf{y} - \\mathbf{m}_x)\n",
    "  - \\frac{1}{2}|\\mathbf{K}_{xx} + \\boldsymbol\\Sigma|\n",
    "  - \\frac{n}{2}\\log (2 \\pi)\n",
    "$$\n",
    "\n",
    "$\\boldsymbol\\Sigma$ is the covariance matrix of the Gaussian noise.  Since the Gaussian noise doesn't need to be white to be conjugate, the `marginal_likelihood` method supports either using a white noise term when a scalar is provided, or a noise covariance function when a covariance function is provided.\n",
    "\n",
    "The `gp.marginal_likelihood` method implements the quantity given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    \n",
    "    sigma = pm.HalfCauchy(\"sigma\", beta=5)\n",
    "    obs = gp.marginal_likelihood(\"obs\", X=X_sim, y=y, sigma=sigma)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    marginal_post = pm.sample(500, tune=1000, chains=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the posterior estimates of the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = az.summary(marginal_post, var_names=[\"l\", \"eta\", \"sigma\"], round_to=2, kind=\"stats\")\n",
    "summary[\"True value\"] = [l_true, eta_true, sigma_true]\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The `.conditional` distribution\n",
    "\n",
    "In addition to fitting the model, we would like to be able to generate predictions. This implies sampling from the posterior predictive distribution, which if you recall is just some linear algebra:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m^*(x^*) &= k(x^*,x)^T[k(x,x) + \\sigma^2I]^{-1}y \\\\\n",
    "k^*(x^*) &= k(x^*,x^*)+\\sigma^2 - k(x^*,x)^T[k(x,x) + \\sigma^2I]^{-1}k(x^*,x)\n",
    "\\end{aligned}$$\n",
    "\n",
    "PyMC allows for predictive sampling after the model is fit, using the recorded values of the model parameters to generate samples. The `conditional` method implements the predictive GP above, called with a grid of points over which to generate realizations:\n",
    "\n",
    "The `.conditional` has an optional flag for `pred_noise`, which defaults to `False`.  When `pred_noise=False`, the `conditional` method produces the predictive distribution for the underlying function represented by the GP.  When `pred_noise=True`, the `conditional` method produces the predictive distribution for the GP plus noise.  \n",
    "\n",
    "If using an additive GP model, the conditional distribution for individual components can be constructed by setting the optional argument `given`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a grid of new values from `x=0` to `x=20`, then add the GP conditional to the model, given the new X values:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = np.linspace(0, 20, 600)[:,None]\n",
    "\n",
    "with model:\n",
    "    f_pred = gp.conditional(\"f_pred\", X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw samples from the posterior predictive distribution over the specified grid of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    pred_samples = pm.sample_posterior_predictive(\n",
    "        marginal_post.sel(draw=slice(0, 20)), var_names=['f_pred']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# plot the samples from the gp posterior with samples and shading\n",
    "from pymc.gp.util import plot_gp_dist\n",
    "f_pred_samples = az.extract(pred_samples, group=\"posterior_predictive\", var_names=[\"f_pred\"])\n",
    "plot_gp_dist(ax, samples=f_pred_samples.T, x=X_new);\n",
    "\n",
    "# plot the data and the true latent function\n",
    "plt.plot(x_sim, f_true, \"dodgerblue\", lw=3, label=\"True f\");\n",
    "plt.plot(x_sim, y, 'ok', ms=3, alpha=0.5, label=\"Observed data\");\n",
    "\n",
    "# axis labels and title\n",
    "plt.xlabel(\"X\"); plt.ylim([-13,13]);\n",
    "plt.title(\"Posterior distribution over $f(x)$ at the observed values\"); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction also matches the results from `gp.Latent` very closely.  What about predicting new data points?  Here we only predicted $f_*$, not $f_*$ + noise, which is what we actually observe.\n",
    "\n",
    "The `conditional` method of `gp.Marginal` contains the flag `pred_noise` whose default value is `False`.  To draw from the *posterior predictive* distribution, we simply set this flag to `True`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with model:\n",
    "    y_pred = gp.conditional(\"y_pred\", X_new, pred_noise=True)\n",
    "    y_samples = pm.sample_posterior_predictive(\n",
    "        marginal_post.sel(draw=slice(0, 20)), var_names=['y_pred']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# posterior predictive distribution\n",
    "y_pred_samples = az.extract(y_samples, group=\"posterior_predictive\", var_names=[\"y_pred\"])\n",
    "plot_gp_dist(ax, y_pred_samples.T, X_new, plot_samples=False, palette=\"bone_r\");\n",
    "\n",
    "# overlay a scatter of one draw of random points from the \n",
    "#   posterior predictive distribution\n",
    "plt.plot(X_new, y_pred_samples.values.T[1], \"co\", ms=2, label=\"Predicted data\");\n",
    "\n",
    "# plot original data and true function\n",
    "plt.plot(X_sim, y, 'ok', ms=3, alpha=1.0, label=\"observed data\");\n",
    "plt.plot(X_sim, f_true, \"dodgerblue\", lw=3, label=\"true f\");\n",
    "\n",
    "plt.xlabel(\"x\"); plt.ylim([-13,13]);\n",
    "plt.title(\"posterior predictive distribution, y_*\"); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-world example: Snowshoe hare population dynamics\n",
    "\n",
    "Let's apply Gaussian processes to a real ecological problem. The plot below shows the relationship between the date and the number of snowshoe hares observed in Kluane, Yukon, Canada. This data comes from long-term monitoring studies of the hare population cycles in the boreal forest ecosystem.\n",
    "\n",
    "Snowshoe hares are known for their dramatic population cycles, typically occurring every 8-11 years. These cycles are driven by complex predator-prey dynamics, particularly with lynx, as well as food availability and other environmental factors. The non-linear nature of these population dynamics makes them an excellent candidate for Gaussian process modeling.\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Lepus_americanus_5459_cropped.jpg/640px-Lepus_americanus_5459_cropped.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the hare population data\n",
    "hare_data = pd.read_csv('../data/hare-data-kluane.csv', parse_dates=['date'])\n",
    "\n",
    "# Convert date to numeric time for GP modeling\n",
    "hare_data['time'] = (hare_data['date'] - hare_data['date'].min()).dt.days / 365.25  # Years since start\n",
    "\n",
    "# Create a rate variable (hares per check) to account for sampling effort\n",
    "hare_data['hare_rate'] = hare_data['# Indiv'] / hare_data['# Checks'].replace(0, 1)  # Avoid division by zero\n",
    "\n",
    "# Plot the raw data\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "ax1.scatter(hare_data['date'], hare_data['# Indiv'], s=50, alpha=0.6, c='darkblue')\n",
    "ax1.set_ylabel('Number of hares observed')\n",
    "ax1.set_title('Snowshoe Hare Population in Kluane, Yukon')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.scatter(hare_data['time'], hare_data['hare_rate'], s=50, alpha=0.6, c='darkgreen')\n",
    "ax2.set_xlabel('Years since study start')\n",
    "ax2.set_ylabel('Hares per check')\n",
    "ax2.set_title('Hare Observation Rate (accounting for sampling effort)')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with zero checks (no sampling effort)\n",
    "hare_data_clean = hare_data[hare_data['# Checks'] > 0].copy()\n",
    "\n",
    "with pm.Model() as hare_model:\n",
    "\n",
    "    # GP hyperparameters\n",
    "    # Lengthscale prior - we expect cycles of 8-11 years based on ecological knowledge\n",
    "    lengthscale = pm.LogNormal('lengthscale', mu=np.log(3), sigma=0.5)\n",
    "    \n",
    "    # Amplitude prior - controls the magnitude of population fluctuations\n",
    "    amplitude = pm.LogNormal('amplitude', mu=np.log(10), sigma=1)\n",
    "    \n",
    "    # Mean function - we'll use a constant mean representing the average hare rate\n",
    "    M = pm.gp.mean.Constant(c=hare_data_clean.hare_rate.mean())\n",
    "    \n",
    "    # Matern 5/2 covariance function - good for capturing smooth but not infinitely differentiable processes\n",
    "    K = (amplitude**2) * pm.gp.cov.Matern52(1, lengthscale) \n",
    "    \n",
    "    # Observation noise\n",
    "    sigma = pm.HalfNormal('sigma', sigma=5)\n",
    "    \n",
    "    # Define the GP\n",
    "    hare_gp = pm.gp.Marginal(mean_func=M, cov_func=K)\n",
    "    \n",
    "    # Likelihood - modeling the hare rate\n",
    "    hare_gp.marginal_likelihood('hare_rate', \n",
    "                                X=hare_data_clean.time.values.reshape(-1,1), \n",
    "                                y=hare_data_clean.hare_rate.values, \n",
    "                                sigma=sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with hare_model:\n",
    "    hare_trace = pm.sample(1000, tune=1000, chains=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(hare_trace, var_names=['lengthscale', 'amplitude', 'sigma'])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction grid - extend slightly beyond observed data to show extrapolation\n",
    "time_pred = np.linspace(hare_data_clean.time.min() - 0.5, \n",
    "                        hare_data_clean.time.max() + 0.5, 200).reshape(-1, 1)\n",
    "\n",
    "with hare_model:\n",
    "    hare_pred = hare_gp.conditional(\"hare_pred\", time_pred)\n",
    "    hare_samples = pm.sample_posterior_predictive(\n",
    "        hare_trace.sel(draw=slice(0, 50)), \n",
    "        var_names=[\"hare_pred\"]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a figure with posterior predictions\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 8))\n",
    "\n",
    "# Plot posterior samples\n",
    "f_pred_samples = az.extract(hare_samples, group=\"posterior_predictive\", var_names=[\"hare_pred\"])\n",
    "plot_gp_dist(ax, f_pred_samples.T, time_pred, palette=\"Reds\", fill_alpha=0.8, samples_alpha=0.03)\n",
    "\n",
    "# Plot observed data\n",
    "ax.scatter(hare_data_clean['time'], hare_data_clean['hare_rate'], \n",
    "           c='black', s=60, alpha=0.8, zorder=10, edgecolors='white', linewidth=1,\n",
    "           label='Observed hare rate')\n",
    "\n",
    "# Add labels and formatting\n",
    "ax.set_xlabel('Years since study start', fontsize=12)\n",
    "ax.set_ylabel('Hares per check', fontsize=12)\n",
    "ax.set_title('Gaussian Process Model of Snowshoe Hare Population Dynamics', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "# Add vertical lines to show the cyclic pattern\n",
    "posterior_mean = f_pred_samples.mean(dim='sample').values\n",
    "peaks, _ = sp.signal.find_peaks(posterior_mean, prominence=5)\n",
    "if len(peaks) > 1:\n",
    "    for peak in peaks:\n",
    "        ax.axvline(time_pred[peak, 0], color='gray', linestyle='--', alpha=0.5, linewidth=1)\n",
    "    \n",
    "    # Calculate average cycle length\n",
    "    if len(peaks) > 1:\n",
    "        cycle_lengths = np.diff(time_pred[peaks, 0])\n",
    "        avg_cycle = np.mean(cycle_lengths)\n",
    "        ax.text(0.02, 0.98, f'Average cycle length: {avg_cycle:.1f} years', \n",
    "                transform=ax.transAxes, fontsize=11, verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Gaussian process model successfully captures the cyclic nature of the snowshoe hare population. A few observations:\n",
    "\n",
    "1. **Lengthscale**: The posterior distribution suggests a lengthscale parameter that aligns well with the known 8-11 year population cycles of snowshoe hares in the boreal forest.\n",
    "\n",
    "2. **Amplitude**: The model captures the dramatic population fluctuations, from near-extinction levels to population booms.\n",
    "\n",
    "3. **Uncertainty quantification**: The GP provides natural uncertainty quantification, with wider credible intervals where data is sparse or at the boundaries of our observations.\n",
    "\n",
    "4. **Smoothness**: The Matérn 5/2 kernel provides an appropriate level of smoothness for biological population data - smooth enough to capture the overall trend but flexible enough to handle the relatively rapid changes during population crashes and recoveries.\n",
    "\n",
    "This example demonstrates how marginal GPs can effectively model complex ecological time series with non-linear dynamics and provide principled uncertainty estimates for both interpolation and short-term extrapolation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `.predict`\n",
    "\n",
    "We can use the `.predict` method to return the mean and variance given a particular `point`.  Since we used `find_MAP` in this example, `predict` returns the same mean and covariance that the distribution of `.conditional` has.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "X_new = np.linspace(0, 20, 600)[:,None]\n",
    "\n",
    "with model:\n",
    "    mu, var = gp.predict(X_new, point=az.extract(marginal_post.posterior.sel(draw=[0], chain=[0])).squeeze(), diag=True)\n",
    "sd = np.sqrt(var)\n",
    "\n",
    "# draw plot\n",
    "fig = plt.figure(figsize=(12,5)); ax = fig.gca()\n",
    "\n",
    "# plot mean and 2sigma intervals\n",
    "plt.plot(X_new, mu, 'r', lw=2, label=\"mean and 2sigma region\");\n",
    "plt.plot(X_new, mu + 2*sd, 'r', lw=1); plt.plot(X_new, mu - 2*sd, 'r', lw=1);\n",
    "plt.fill_between(X_new.flatten(), mu - 2*sd, mu + 2*sd, color=\"r\", alpha=0.5)\n",
    "\n",
    "# plot original data and true function\n",
    "plt.plot(X_sim, y, 'ok', ms=3, alpha=1.0, label=\"observed data\");\n",
    "plt.plot(X_sim, f_true, \"dodgerblue\", lw=3, label=\"true f\");\n",
    "\n",
    "plt.xlabel(\"x\"); plt.ylim([-13,13]);\n",
    "plt.title(\"predictive mean and 2sigma interval\"); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Variable Implementation\n",
    "\n",
    "The `gp.Latent` class is a more general implementation of a GP.  It is called \"Latent\" because the underlying function values are treated as latent variables.  It has a `prior` method, and a `conditional` method.  Given a mean and covariance function, the function $f(x)$ is modeled as,\n",
    "\n",
    "$$\n",
    "f(x) \\sim \\mathcal{GP}(m(x),\\, k(x, x')) \\,.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `.prior`\n",
    "\n",
    "With some data set of finite size, the `prior` method places a multivariate normal prior distribution on the vector of function values, $\\mathbf{f}$,\n",
    "\n",
    "$$\n",
    "\\mathbf{f} \\sim \\text{MvNormal}(\\mathbf{m}_{x},\\, \\mathbf{K}_{xx}) \\,,\n",
    "$$\n",
    "\n",
    "where the vector $\\mathbf{m}$ and the matrix $\\mathbf{K}_{xx}$ are the mean vector and covariance matrix evaluated over the inputs $x$.  \n",
    "\n",
    "By default, PyMC reparameterizes the prior on `f` under the hood by rotating it with the Cholesky factor of its covariance matrix.  This helps to reduce covariances in the posterior of the transformed random variable, `v`.  The reparameterized model is,\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "  \\mathbf{v} \\sim \\text{N}(0, 1)& \\\\\n",
    "  \\mathbf{L} = \\text{Cholesky}(\\mathbf{K}_{xx})& \\\\\n",
    "  \\mathbf{f} = \\mathbf{m}_{x} + \\mathbf{Lv} \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This reparameterization can be disabled by setting the optional flag in the `prior` method, `reparameterize = False`.  The default is `True`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson regession\n",
    "\n",
    "Let's revisit the hare data. Since we're dealing with count data (number of hares observed), a Poisson likelihood is more appropriate than the normal likelihood used in the marginal GP implementation.\n",
    "\n",
    "We'll model the hare count data using a latent Gaussian process with a Poisson likelihood. The latent GP represents the underlying log-intensity of the Poisson process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the model in PyMC.  We use a $\\text{Gamma}(2, 1)$ prior over the lengthscale parameter, and weakly informative $\\text{HalfCauchy}(2)$ priors over the covariance function scale, and noise scale.  A $\\text{Gamma}(2, 0.1)$ prior is assigned to the degrees of freedom parameter of the noise.  Finally, a GP prior is placed on the unknown function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hare = hare_data_clean['time'].values[:, None]\n",
    "y_hare = hare_data_clean['# Indiv'].values\n",
    "\n",
    "with pm.Model() as hare_poisson_model:\n",
    "\n",
    "    lengthscale = pm.InverseGamma('lengthscale', alpha=2, beta=1)\n",
    "    amplitude = pm.HalfNormal('amplitude', sigma=10)\n",
    "    \n",
    "    cov = (amplitude**2) * pm.gp.cov.Matern52(1, lengthscale)\n",
    "    \n",
    "    gp = pm.gp.Latent(cov_func=cov)\n",
    "    \n",
    "    f = gp.prior('f', X=X_hare)\n",
    "    \n",
    "    pm.Poisson('y_obs', mu=pm.math.exp(f), observed=y_hare)\n",
    "    \n",
    "    hare_poisson_trace = pm.sample(draws=1000, tune=500, nuts_sampler='nutpie', chains=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that I snuck in a new argument to `sample`:\n",
    "\n",
    "`nuts_sampler='nutpie'`\n",
    "\n",
    "While the latent variable implementation makes Gaussian processes really flexible, it also comes with a computational cost since we are no longer able to work witht the marginal likelihood directly. So, its nice to be able to take advantage of some of the accelerated sampling methods available in PyMC.\n",
    "\n",
    "> ### Accelerated Sampling in PyMC with Numba and Jax\n",
    "> \n",
    "> By default, PyMC is using the C backend to compile its models, which then gets \n",
    "> called by the Python-based samplers. PyMC also offers accelerated sampling \n",
    "> through the use of Numba and Jax via the Numpyro and Nutpie libraries. \n",
    "> These libraries provide just-in-time (JIT) compilation and GPU acceleration, \n",
    "> which can significantly speed up the sampling process.\n",
    "> \n",
    "> **Numba** is a JIT compiler that translates a subset of Python and NumPy code \n",
    "> into fast machine code. It can be used to accelerate the execution of PyMC models by \n",
    "> compiling the log-probability function.\n",
    "> \n",
    "> **Jax** is a library for high-performance numerical computing that provides \n",
    "> automatic differentiation and GPU acceleration. It can be used with PyMC to speed up \n",
    "> the sampling process by leveraging Jax's capabilities.\n",
    "> \n",
    "> **Numpyro** is a probabilistic programming library built on Jax. It provides \n",
    "> a high-level interface for defining and sampling from probabilistic models. PyMC can \n",
    "> use Numpyro as a backend to take advantage of Jax's acceleration.\n",
    "> \n",
    "> **Nutpie** is a library that provides an optimized implementation of the \n",
    "> No-U-Turn Sampler (NUTS) algorithm. It can be used with PyMC to perform efficient \n",
    "> sampling.\n",
    "> \n",
    "> To use these libraries with PyMC, you can specify the `nuts_sampler` argument in the `pm.sample` function, using either `\"nutpie\"` or `\"numpyro\"`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the posteriors of the covariance function hyperparameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(hare_poisson_trace, var_names=['lengthscale', 'amplitude'], backend_kwargs=dict(constrained_layout=True));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_pred = np.linspace(hare_data_clean.time.min() - 0.5, \n",
    "                        hare_data_clean.time.max() + 0.5, 200)[:, None]\n",
    "\n",
    "with hare_poisson_model:\n",
    "\n",
    "    f_pred = gp.conditional('f_pred', time_pred)\n",
    "    \n",
    "    lambda_pred = pm.Deterministic('lambda_pred', pm.math.exp(f_pred))\n",
    "    \n",
    "    hare_pred_samples = pm.sample_posterior_predictive(\n",
    "        hare_poisson_trace.sel(draw=slice(0, 50)), \n",
    "        var_names=['f_pred', 'lambda_pred']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(14, 8))\n",
    "\n",
    "lambda_pred_samples = az.extract(hare_pred_samples, group=\"posterior_predictive\", var_names=[\"lambda_pred\"])\n",
    "\n",
    "max_observed = np.max(y_hare)\n",
    "y_max = min(max_observed * 1.5, np.percentile(lambda_pred_samples.values, 95))\n",
    "\n",
    "plot_gp_dist(ax, lambda_pred_samples.T, time_pred, \n",
    "             palette=\"Reds\", fill_alpha=0.8, samples_alpha=0.03)\n",
    "\n",
    "ax.scatter(hare_data_clean['time'], y_hare, \n",
    "          c='black', s=60, alpha=0.8, zorder=10, edgecolors='white', linewidth=1,\n",
    "          label='Observed hare counts')\n",
    "\n",
    "ax.set_xlabel('Years since study start', fontsize=12)\n",
    "ax.set_ylabel('Number of hares', fontsize=12)\n",
    "ax.set_title('Poisson GP Model of Snowshoe Hare Population Dynamics', fontsize=14)\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=11)\n",
    "\n",
    "ax.set_ylim(0, 80);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The latent variable implementation is more computationally intensive than the marginal GP approach but offers much greater flexibility in terms of likelihood choices, making it ideal for non-Gaussian data like counts, binary outcomes, or other exponential family distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster GPs: Hilbert Space Approximate Gaussian Processes (HSGP)\n",
    "\n",
    "The Hilbert Space Gaussian processes approximation is a **low-rank GP approximation** that is particularly well-suited to usage in probabilistic programming languages.  It approximates the GP using a pre-computed and fixed set of basis functions that don't depend on the form of the covariance kernel or its hyperparameters.  It's a _parametric_ approximation, so prediction can be done as one would with any other type of Bayesian model in PyMC;  You don't need to define the `.conditional` distribution that the `Marginal` and `Latent` GPs rely on.  This makes it _much_ easier to integrate an HSGP into your existing model as they can be used anywhere within a model and with any likelihood function.  \n",
    "\n",
    "It's also fast! 🏃 The computational cost for standard GPs per MCMC step is $\\mathcal{O}(n^3)$, where $n$ is the number of data points.  For HSGPs, it is $\\mathcal{O}(mn + m)$, where $m$ is the number of basis vectors. \n",
    "\n",
    "The HSGP approximation does carry some caveats:\n",
    "1. It can only be used with _stationary_ covariance kernels such as the Matern family.  The `HSGP` class is compatible with any `Covariance` class that implements the `power_spectral_density` method.  There is a special case made for the `Periodic` covariance, which is implemented in PyMC by `HSGPPeriodic`. \n",
    "2. It does not scale well with the input dimension.  The HSGP approximation is a good choice if your GP is over a one dimensional process like a time series, or a two dimensional spatial point process.  It's likely not an efficient choice where the input dimension is larger than three. \n",
    "3. It _may_ struggle with more rapidly varying processes.  If the process you're trying to model changes very quickly relative to the extent of the domain, the HSGP approximation may fail to accurately represent it.  We'll show in later sections how to set the accuracy of the approximation, which involves a trade-off between the fidelity of the approximation and the computational complexity.\n",
    "4. For smaller data sets, the full unapproximated GP may still be more efficient.\n",
    "\n",
    "A secondary goal of this implementation is **flexibility** via an accessible implementation where the core computations are implemented in a modular way. This results in both a \"high-level\" interface where users can use the familiar `.prior` and `.conditional` methods and essentially treat the HSGP class as a drop in replacement for `pm.gp.Latent`, and a \"low-level\" interface that exposes the HSGP as a parametric model.  For more complex models with multiple HSGPs, users can work directly with functions like `pm.gp.hsgp_approx.calc_eigenvalues` and `pm.gp.hsgp_approx.calc_eigenvectors`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work?\n",
    "\n",
    "In HSGP, the covariance function is viewed as a mathematical operator acting on a high-dimensional space (Hilbert space). We represent covariance function as a series expansion of eigenvalues and eigenfunctions of the Laplace operator. These eigenfunctions capture the smoothness properties of the underlying function being modeled in the form of a set of basis functions.\n",
    "\n",
    "$$\n",
    "f \\sim \\mathcal{G P}\\left(0, k\\left(x, x^{\\prime} ; \\ell\\right)\\right) \\longrightarrow f \\approx \\phi(x) \\beta(\\ell)\n",
    "$$\n",
    "\n",
    "where the basis functions $\\phi$ only depend on the input and the coefficients $\\beta$ only depend on the kernel hyperparmeters.\n",
    "\n",
    "By approximating the covariance function with a set of basis functions, HSGP reduces the complexity significantly, from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(nm + m)$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cherry blossom data set\n",
    "\n",
    "The cherry blossom dataset contains historical records of cherry tree blooming in Japan dating back to 801 CE. It includes:\n",
    "\n",
    "- `year`: The year of observation (801-2015)\n",
    "- `doy`: Day of year when cherry trees first bloomed (measured in days)\n",
    "- `temp`: Reconstructed temperature data (in Celsius)\n",
    "- `temp_upper` and `temp_lower`: Upper and lower bounds of temperature estimates\n",
    "\n",
    "The dataset shows considerable annual variation in bloom timing, and is valuable for studying climate patterns and phenological responses over an exceptionally long time period, making it useful for Gaussian Process modeling and other time series analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(pm.get_data(\"../data/cherry_blossoms.csv\"), sep=\";\")\n",
    "df = df[[\"year\", \"doy\", \"temp\"]].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 1, figsize=(10, 7)); axs = axs.flatten()\n",
    "\n",
    "axs[0].scatter(df[\"year\"], df[\"temp\"])\n",
    "axs[1].scatter(df[\"year\"], df[\"doy\"])\n",
    "\n",
    "axs[1].set_xlabel(\"year\")\n",
    "axs[0].set_ylabel(\"temp (C)\")\n",
    "axs[1].set_ylabel(\"day of the first bloom\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is the same as with any other GP model: we need to specify a covariance function. We will use the Matern 5/2 covariance function, which is a good default choice for many problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = {\n",
    "    \"year\": df.year.tolist()\n",
    "}\n",
    "with pm.Model(coords=coords) as cherry_blossom_model:\n",
    "    # Set prior on GP hyperparameters\n",
    "    eta = pm.Exponential(\"eta\", lam=0.25)\n",
    "\n",
    "    ell = pm.InverseGamma('ell', 10, 500)\n",
    "    \n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, ls=ell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to specify the HSGP model. We will use the `HSGP` class, which is a drop-in replacement for the `gp.Latent` class. \n",
    "\n",
    "There are three important hyperparameters for `HSGP`:\n",
    "\n",
    "- `m`: The **number of basis vectors** to use for each active dimension.\n",
    "- `L`: The **boundary of the space** for each active_dim. It is called the boundary condition. Choose `L` such that the domain `[-L, L]` contains all points in the column of `X` given by the active dimension.\n",
    "- `c`: The **proportion extension factor**. Used to construct `L` from `X`. Defined as $S = \\max|X| \\ni X \\in [-S, S]$. L is calculated as `c * S`. One of `c` or `L` must be provided. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Choosing the HSGP approximation parameters\n",
    "\n",
    "Before fitting a model with an HSGP, you have to choose `m` and `c` or `L`.  `m` is the number of basis vectors.  Recall that the computational complexity of the HSGP approximation is $\\mathcal{O}(mn + m)$, where $n$ is the number of data points. \n",
    "\n",
    "This choice is a balance between three concerns:\n",
    "1.  The accuracy of the approximation.\n",
    "2.  Reducing the computational burden.\n",
    "3.  The `X` locations where predictions or forecasts will need to be made.\n",
    "\n",
    "The best way to understand how to choose these parameters is to understand how `m`, `c` and `L` relate to each other, which requires understanding a bit more about how the approximation works under the hood.  \n",
    "\n",
    "### How `L` and `c` affect the basis\n",
    "\n",
    "The HSGP approximates the GP prior as a **linear combination of sinusoids**.  The coefficients of the linear combination are IID normal random variables whose standard deviation depends on GP hyperparameters (which are an amplitude and lengthscale for the Matern family). \n",
    "\n",
    "To see this, we'll make a few plots of the $m=3$ basis vectors and pay careful attention to how they behave at the boundaries of the domain.  Note that we have to center the `x` data first, and then choose `L` in relation to the centered data.  It's worth mentioning here that the basis vectors we're plotting do not depend on either the choice of the covariance kernel or on any unknown parameters the covariance function has.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our data goes from x=-5 to x=5\n",
    "x = np.linspace(-5, 5, 1000)\n",
    "\n",
    "# (plotting code)\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 4))\n",
    "plt.subplots_adjust(wspace=0.02)\n",
    "ylim = 0.55\n",
    "axs[0].set_ylim([-ylim, ylim])\n",
    "axs[1].set_yticks([])\n",
    "axs[1].set_ylim([-ylim, ylim])\n",
    "axs[1].set_xlabel(\"xs (mean subtracted x)\")\n",
    "axs[1].set_title(\"The effect of changing $L$ on the HSGP basis vectors\")\n",
    "axs[2].set_yticks([])\n",
    "axs[2].set_ylim([-ylim, ylim])\n",
    "\n",
    "# change L as we create the basis vectors\n",
    "L_options = [5.0, 6.0, 20.0]\n",
    "m_options = [3, 3, 5]\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    L = L_options[i]\n",
    "    m = m_options[i]\n",
    "\n",
    "    eigvals = pm.gp.hsgp_approx.calc_eigenvalues(pt.as_tensor([L]), [m])\n",
    "    phi = pm.gp.hsgp_approx.calc_eigenvectors(\n",
    "        x[:, None], pt.as_tensor([L]), eigvals, [m]\n",
    "    ).eval()\n",
    "\n",
    "    colors = plt.cm.cividis_r(np.linspace(0.05, 0.95, m))\n",
    "    for j in range(phi.shape[1]):\n",
    "        ax.plot(x, phi[:, j], color=colors[j], label=f\"eigenvector {j+1}\")\n",
    "\n",
    "    ax.set_xticks(np.arange(-5, 6, 5))\n",
    "\n",
    "    S = 5.0\n",
    "    c = L / S\n",
    "    ax.text(-4.9, -0.45, f\"L = {L}\\nc = {c}\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that both `L` and `m` are specified as lists, to allow setting `L` and `m` per input dimension.  In this example these are both one element lists since our example is in a one dimensional, time series like context.  Before continuing, it's helpful to define $S$ as the half range of the centered data, or the distance from the midpoint at $x=0$ to the edge, $x=5$.  In this example $S=5$ for each plot panel.  Then, we can define $c$ such that it relates $S$ to $L$, \n",
    "\n",
    "$$\n",
    "L = c \\cdot S \\,.\n",
    "$$\n",
    "It's usually easier to set $L$ by choosing $c$, which acts as a multiplier on $S$.  \n",
    "\n",
    "In the left-most plot we chose $L=S=5$, which is exactly on the edge of our `x` locations.  For any $m$, all the basis vectors are forced to pinch to zero at the edges, at $x=-5$ and $x=5$.  This means that the HSGP approximation becomes poor as you get closer to $x=-5$ and $x=5$.  How quickly depends on the lengthscale.  Large lengthscales require larger values of $L$ and $c$, and smaller lengthscales attenuate this issue.  Ruitort-Mayol *et al.* recommend using 1.2 as a minimum value.  The effect of this choice on the basis vectors is shown in the center panel.  \n",
    "\n",
    "The right panel shows the effect of choosing a larger $L$, or setting $c=4$.  Larger values of $L$ or $c$ make the boundary conditions less problematic, and are required to accurately approximate GPs with longer lengthscales.  You also need to consider where predictions will need to be made.  In addition to the locations of the observed $x$ values, the locations of the new $x$ locations also need to be away from the \"pinch\" caused by the boundary condition.  The _period_ of the basis functions also increases as we increase $L$ or $c$.  This means that we will need to increase $m$ in order to compensate if we wish to approximate GPs with smaller lengthscales.  \n",
    "\n",
    "With large $L$ or $c$, the first eigenvector can flatten so much that it becomes partially or completely unidentifiable with the intercept in the model.  The right-most panel is an example of this.  It can be very beneficial to sampling to drop the first eigenvector in these situations.  The `HSGP` and `HSGPPeriodic` class in PyMC both have the option `drop_first` to do this, or if you're using `.prior_linearized` you can control this yourself.  Be sure to check the basis vectors if the sampler is having issues.\n",
    "\n",
    "To summarize:\n",
    "\n",
    "- Increasing $m$ helps the HSGP approximate GPs with smaller lengthscales, at the cost of increasing computational complexity\n",
    "- Increasing $c$ or $L$ helps the HSGP approximate GPs with larger lengthscales, but may require increasing $m$ to compensate for the loss of fidelity at smaller lengthscales.  \n",
    "- When choosing $m$, $c$ or $L$, it's important to consider the locations where you will need to make predictions, such that they also aren't affected by the boundary condition.\n",
    "- The first eigenvector in the basis may be unidentified with the intercept, especially when $L$ or $c$ are larger. \n",
    "\n",
    "\n",
    "### Heuristics for choosing $m$ and $c$\n",
    "\n",
    "In practice, you'll need to infer the lengthscale from the data, so the HSGP needs to approximate a GP across a range of lengthscales that are representative of your chosen prior.  You'll need to choose $c$ large enough to handle the largest lengthscales you might fit, and also choose $m$ large enough to accommodate the smallest lengthscales.  Ruitort-Mayol *et al.* give some handy heuristics for the range of lengthscales that are accurately reproduced for given values of $m$ and $c$.  The plot below uses their heuristics to recommend minimum $m$ and $c$ value.  Note that these recommendations are based on a one-dimensional GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MultipleLocator\n",
    "\n",
    "c_list = np.array([1.2, 1.3, 1.5, 1.75, 2.0, 2.5, 3.0, 4.0, 5.0, 6.0])\n",
    "ell = np.linspace(1, 1000, 500)\n",
    "S = (df[\"year\"] - df[\"year\"].mean()).max() # half-range of the input data\n",
    "\n",
    "fig = plt.figure(figsize=(10, 5))\n",
    "ax = fig.gca()\n",
    "\n",
    "cmap = plt.cm.cividis\n",
    "colors = np.arange(len(c_list)) / len(c_list)\n",
    "\n",
    "for i, c in enumerate(c_list):\n",
    "    m = 2.65 * (c / ell) * S\n",
    "   \n",
    "    ix = c >= (4.1 * (ell / S))\n",
    "    m[~ix] = np.nan\n",
    "    ax.semilogy(ell, m, color=cmap(colors[i]), label=\"c = %s\" % str(c), lw=3);\n",
    "\n",
    "ax.grid(True);\n",
    "\n",
    "ax.xaxis.set_major_locator(MultipleLocator(50))\n",
    "ax.xaxis.set_minor_locator(MultipleLocator(10))\n",
    "\n",
    "ax.set_title(\"Matern52 HSGP approx parameter curves\");\n",
    "ax.set_ylim([10, 1000]);\n",
    "ax.set_xlabel(\"lengthscale (ℓ)\");\n",
    "ax.set_ylabel(\"number of basis vectors (m)\");\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The curves show the regions where the HSGP approximation is accurate.  For instance, the darkest blue line at $c=1.2$ means that if we choose $c=1.2$, our approximation will be valid for the smallest lengthscales up until about $\\ell = 130$.  The yellow curve at $c=6.0$ means shows that the approximation is accurate up until about $\\ell = 650$.  **How we choose these values depends on our prior for the lengthscale**.  In our case we are setting 95% of the prior mass between 20 and 200.  We'll be safe and choose $c=2$ and $m=400$ so our prior range is well covered pretty well.  \n",
    "\n",
    "Also, keep in mind that HSGP scales as $\\mathcal{O}(nm + m)$, so the smaller we can choose $m$, the better, speed-wise.  $c$ plays no role in computation speed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using `HSGP` in PyMC\n",
    "\n",
    "There are a couple of usage details that are important for using `HSGP` effectively. The first is that, due to how the basis vectors are constructed, the `HSGP` class requires that the `X` input data be centered.  This is because the basis vectors are constructed to be periodic over the range $[-L, L]$, and the center of the range is assumed to be zero.  \n",
    "\n",
    "So, here we will center the age range on the mean age, taking care to use the closest integer value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "years = df['year'].values\n",
    "years_centered = years - years[len(years) // 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cherry_blossom_model:\n",
    "\n",
    "    gp = pm.gp.HSGP(m=[400], c=2.0, cov_func=cov, parametrization='non-centered')\n",
    "    f = gp.prior(\"f\", X=years_centered[:, None], dims=\"year\")\n",
    "    \n",
    "    intercept = pm.Normal(\"intercept\", mu=df[\"temp\"].mean(), sigma=2 * df[\"temp\"].std())\n",
    "    mu = pm.Deterministic(\"mu\", intercept + f, dims=\"year\")\n",
    "    \n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=2 * df[\"temp\"].std())\n",
    "    pm.Normal(\"y\", mu=mu, sigma=sigma, observed=df[\"temp\"].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that in the specification of `HSGP` the parameterization was set to **non-centered**. This relates to how the power spectrum is entered into the model. Here is what happens under the hood:\n",
    "\n",
    "First, `prior_linearized` returns the eigenvector basis, `phi`, and the square root of the power spectrum at the eigenvalues, `sqrt_psd`.  You have to construct the HSGP approximation from these.  The following are the relevant lines of code, showing both the centered and non-centered parameterization.\n",
    "\n",
    "```python\n",
    "phi, sqrt_psd = gp.prior_linearized(Xs=Xs)\n",
    "\n",
    "## non-centered\n",
    "beta = pm.Normal(\"beta\", size=gp._m_star)\n",
    "f = pm.Deterministic(\"f\", phi @ (beta * sqrt_psd)) \n",
    "\n",
    "## centered\n",
    "beta = pm.Normal(\"beta\", sigma=sqrt_psd, size=gp._m_star)\n",
    "f = pm.Deterministic(\"f\", phi @ beta) \n",
    "```\n",
    "\n",
    "Choosing a non-centered parameterization may help alleviate sampling issues, but is not always necessary, particularlty if there are plenty of data informing the GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with cherry_blossom_model:\n",
    "    trace_gp = pm.sample(nuts_sampler='nutpie', chains=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(trace_gp, var_names=[\"eta\", \"ell\", \"sigma\", \"intercept\"])\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This took a few minutes on my machine. NUTS depends a ton on the posterior geometry, and sampling the GP hyperparameters has a huge influence, but the speeds are still comparable.  We also see a lengthscale of about 40, which we can interpret as meaning that it takes about 40 years for significant changes in temperature to occur.  We can also interpret this to roughly mean that the GP is comfortable predicting out about 40 years.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(18, 10));\n",
    "\n",
    "f = az.extract(trace_gp, group=\"posterior\", var_names=\"mu\")\n",
    "pm.gp.util.plot_gp_dist(ax=ax, samples=f.values.T, x=df[\"year\"].values)\n",
    "ax.plot(df[\"year\"].values, df[\"temp\"].values, \"ok\", alpha=0.25);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The HSGP approximation does carry some restrictions:\n",
    "1. It **can only be used with _stationary_ covariance kernels** such as the Matern family.  The `HSGP` class is compatible with any `Covariance` class that implements the `power_spectral_density` method.  There is a special case made for the `Periodic` covariance, which is implemented in PyMC by The `HSGPPeriodic`.\n",
    "2. It **does not scale well with the input dimension**.  The HSGP approximation is a good choice if your GP is over a one dimensional process like a time series, or a two dimensional spatial point process.  It's likely not an efficient choice where the input dimension is larger than three. \n",
    "3. It **_may_ struggle with more rapidly varying processes**.  If the process you're trying to model changes very quickly relative to the extent of the domain, the HSGP approximation may fail to accurately represent it.  We'll show in later sections how to set the accuracy of the approximation, which involves a trade-off between the fidelity of the approximation and the computational complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking Forward\n",
    "\n",
    "This course has introduced the fundamentals of Bayesian time series \n",
    "analysis with PyMC, but there's a vast landscape of advanced techniques \n",
    "and applications to explore. Here are some directions for further study:\n",
    "\n",
    "### Advanced Time Series Models in PyMC\n",
    "\n",
    "**State-Space Models**\n",
    "\n",
    "The [pymc-extras](https://github.com/pymc-devs/pymc-extras) \n",
    "package includes a powerful `statespace` module that implements classical\n",
    "    time series models in a Bayesian framework:\n",
    "- **ARIMA models** with automatic differencing and seasonal components\n",
    "- **Structural time series models** (local level, local linear trend, \n",
    "etc.)\n",
    "- **Dynamic linear models** for time-varying parameters\n",
    "- **Kalman filtering and smoothing** for efficient inference\n",
    "- Blog post: [Time Series Models Derived From a Generative Graph](https://www.pymc.io/projects/examples/en/latest/time_series/Time_Series_Generative_Graph.html)\n",
    "\n",
    "**Vector Autoregression (VAR) and Bayesian VARs** \n",
    "\n",
    "For multivariate time series:\n",
    "- Model interdependencies between multiple time series\n",
    "- Implement Bayesian VARs with informative priors (Minnesota prior, etc.)\n",
    "- Tutorial: [Bayesian Vector Autoregression Models](https://www.pymc.io/projects/examples/en/latest/time_series/bayesian_var_model.html)\n",
    "\n",
    "\n",
    "### Advanced Gaussian Process Techniques\n",
    "\n",
    "**Multi-output GPs**  \n",
    "\n",
    "Model multiple related time series simultaneously:\n",
    "- Share information across outputs through kernel design\n",
    "- Applications in multi-sensor data and spatial-temporal modeling\n",
    "- Tutorial: [Multi-output Gaussian Processes: Coregionalization models using Hamadard product](https://www.pymc.io/projects/examples/en/latest/gaussian_processes/MOGP-Coregion-Hadamard.html)\n",
    "\n",
    "**Advanced HSGP Models**  \n",
    "\n",
    "Additional coverage of the powerful approximate GP model:\n",
    "- Pre-computed basis functions\n",
    "- Custom GP models, like hierarchical GPs and models that utilize Kronecker structure\n",
    "- Tutorial: [Gaussian Processes: HSGP Advanced Usage](https://www.pymc.io/projects/examples/en/latest/gaussian_processes/HSGP-Advanced.html)\n",
    "\n",
    "### Specialized Time Series Applications\n",
    "\n",
    "**Stochastic Differential Equations (SDEs)** \n",
    "\n",
    "Model continuous-time processes:\n",
    "- Financial models \n",
    "- Population dynamics\n",
    "- Neural dynamics\n",
    "- Tutorial: [Lotka-Volterra with manual gradients](https://www.pymc.io/projects/examples/en/latest/ode_models/ODE_with_manual_gradients.html)\n",
    "\n",
    "**Change Point Detection**  \n",
    "\n",
    "Identify regime changes in time series:\n",
    "- Multiple change points with reversible jump MCMC\n",
    "- Online change point detection\n",
    "- Example: [Interrupted Time Series Analysis](https://www.pymc.io/projects/examples/en/latest/time_series/interrupted_time_series.html)\n",
    "\n",
    "**Hierarchical Time Series**  \n",
    "\n",
    "Model grouped or nested time series:\n",
    "- Partial pooling across groups\n",
    "- Time-varying group effects\n",
    "- Applications in demand forecasting and epidemiology\n",
    "- Video: [Hierarchical Time Series With Prophet and PyMC3 by Matthijs Brouns](https://www.youtube.com/watch?v=appLxcMLT9Y)\n",
    "\n",
    "### Resources for Continued Learning\n",
    "\n",
    "**Online Courses and Tutorials**\n",
    "\n",
    "- [PyMC Examples Gallery](https://www.pymc.io/projects/examples/en/latest/gallery.html) - \n",
    "Extensive collection of notebooks\n",
    "- [Stan User's Guide Time Series Chapter](https://mc-stan.org/docs/stan-users-guide/time-series.html) - \n",
    "Many concepts transfer to PyMC\n",
    "\n",
    "**Community and Support**\n",
    "\n",
    "- [PyMC Discourse Forum](https://discourse.pymc.io/) - Active community \n",
    "for questions\n",
    "- [PyMC GitHub Discussions](https://github.com/pymc-devs/pymc/discussions)\n",
    "- [PyMC Labs Blog](https://www.pymc-labs.com/blog-posts/) - Advanced \n",
    "applications and case studies\n",
    "\n",
    "**Key PyMC Extensions**\n",
    "\n",
    "- `pymc-extras`: Cutting-edge models including state-space \n",
    "implementations\n",
    "- `bambi`: High-level interface for mixed models with time series \n",
    "components\n",
    "- `pymc-bart`: Bayesian Additive Regression Trees for non-parametric time\n",
    "    series\n",
    "\n",
    "The Bayesian approach to time series modeling offers unique \n",
    "advantages: principled uncertainty quantification, the ability to \n",
    "incorporate domain knowledge through priors, and a unified framework for \n",
    "model comparison. As you continue your journey, focus on matching the \n",
    "model complexity to your problem's needs and always validate your models \n",
    "through posterior predictive checks and out-of-sample evaluation.\n",
    "\n",
    "The future of Bayesian time series analysis is bright, with ongoing \n",
    "developments in scalable inference algorithms, integration with deep \n",
    "learning, and applications to increasingly complex real-world problems. \n",
    "Happy modeling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyMC Code Sprint 🏃‍♀️🏃‍➡️\n",
    "\n",
    "🗓️ Saturday, June 8 10:20 - 12:35 Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## References\n",
    "\n",
    "[Rasmussen, C. E., & Williams, C. K. I. (2005). Gaussian Processes for Machine Learning (Adaptive Computation and Machine Learning series). The MIT Press.](http://www.amazon.com/books/dp/026218253X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
