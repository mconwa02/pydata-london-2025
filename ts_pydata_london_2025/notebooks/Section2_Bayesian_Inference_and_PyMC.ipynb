{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2: Bayesian Inference and PyMC\n",
    "\n",
    "#### PyData London 2025 - Bayesian Time Series Analysis with PyMC\n",
    "\n",
    "---\n",
    "\n",
    "## Statistical Paradigms: Two Ways of Thinking\n",
    "\n",
    "Before diving into Bayesian methods, it's crucial to understand how Bayesian thinking differs from the traditional frequentist approach.\n",
    "\n",
    "### Frequentist Worldview\n",
    "\n",
    "![Fisher](images/fisher.png)\n",
    "\n",
    "The **frequentist approach** to statistics treats data and parameters in a fundamentally different way than Bayesian methods. In this framework, **data are considered random** because they vary each time we collect them from the same underlying process. Conversely, **parameters are treated as fixed** but unknown constants that we seek to estimate. This leads to the mathematical formulation where we condition on parameters: \n",
    "\n",
    "$$\\Large P(\\text{data} | \\text{parameters}) $$\n",
    "\n",
    "Frequentist inference operates through **point estimators** and **confidence intervals**, where the interpretation of uncertainty relates to the long-run frequency properties of our estimation procedures. A 95% confidence interval, for example, means that if we repeated our sampling procedure many times, 95% of the intervals we construct would contain the true parameter value.\n",
    "\n",
    "### Bayesian Worldview  \n",
    "\n",
    "![Bayes](images/bayes.png)\n",
    "\n",
    "The **Bayesian paradigm** flips this perspective: Here, **data are treated as fixed** once we have observed them—they represent the concrete evidence we have gathered. In contrast, **parameters are treated as random variables** about which we express our uncertainty using probability distributions. This leads to conditioning on the observed data: \n",
    "\n",
    "$$\\Large P(\\text{parameters} | \\text{data}) $$\n",
    "\n",
    "Bayesian inference proceeds through **probability distributions** that directly quantify our uncertainty about parameter values. A Bayesian 95% credible interval has the intuitive interpretation that there is a 95% probability that the parameter lies within that interval, given our data and model assumptions.\n",
    "\n",
    "This fundamental philosophical difference leads to more **intuitive interpretations** and **natural uncertainty quantification**, making Bayesian methods particularly appealing for time series analysis where we often want to make probabilistic statements about future observations.\n",
    "\n",
    "---\n",
    "\n",
    "## Bayes' Theorem: The Foundation\n",
    "\n",
    "**Bayes' Theorem** is our single tool for learning from data:\n",
    "\n",
    "$$\\huge\n",
    "\\underbrace{\\text{Pr}(\\theta | y)}_{\\textcolor{yellow}{\\small \\text{Posterior Probability}}}\n",
    "= \n",
    "\\frac{\n",
    "\\overbrace{\\text{Pr}(y | \\theta)}^{\\textcolor{yellow}{\\small \\text{Data Likelihood}}} \\cdot\n",
    "\\overbrace{\\text{Pr}(\\theta)}^{\\textcolor{yellow}{\\small \\text{Prior Probability}}}\n",
    "}{\n",
    "\\underbrace{\\text{Pr}(y)}_{\\textcolor{yellow}{\\small \\text{Normalizing Constant}}}\n",
    "}\n",
    "$$\n",
    "\n",
    "### Breaking Down the Components\n",
    "\n",
    "- **$P(\\theta | y)$** = **Posterior**: What we learn about parameters after seeing data\n",
    "- **$P(y | \\theta)$** = **Likelihood**: How well different parameter values explain our data  \n",
    "- **$P(\\theta)$** = **Prior**: Our initial beliefs about parameters before seeing data\n",
    "- **$P(y)$** = **Evidence**: Normalizing constant (usually intractable)\n",
    "\n",
    "### The Learning Process\n",
    "\n",
    "Bayesian inference is a formal process of **updating beliefs**:\n",
    "\n",
    "1. **Start** with prior beliefs $P(\\theta)$\n",
    "2. **Observe** data and compute likelihood $P(y | \\theta)$\n",
    "3. **Update** to posterior beliefs $P(\\theta | y)$\n",
    "4. **Repeat** as more data arrives\n",
    "\n",
    "This makes Bayesian methods naturally suited for time series, where we continuously update our understanding as new observations arrive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Specification: Encoding Our Beliefs\n",
    "\n",
    "One of the most important aspects of Bayesian modeling is **prior specification**. Priors encode our beliefs about parameters before seeing data.\n",
    "\n",
    "### Types of Priors\n",
    "\n",
    "The choice of prior distribution represents one of the most important decisions in Bayesian modeling, as it directly influences both the computational efficiency and the interpretability of results. Understanding the different types of priors and their appropriate applications is essential for effective Bayesian analysis.\n",
    "\n",
    "**Informative priors** incorporate substantial domain knowledge or previous research findings into the analysis. These priors express strong beliefs about parameter values based on external information. For example, if we know from demographic research that birth rates typically range between 10-20 per 1000 people, we might specify `pm.Normal('rate', mu=15, sigma=2)` to encode this knowledge. Informative priors are particularly valuable when working with limited data or when we want to incorporate expert knowledge into our analysis.\n",
    "\n",
    "**Weakly informative priors** provide gentle regularization without imposing strong constraints on the parameter space. These priors help stabilize the estimation process while allowing the data to largely determine the posterior distribution. A common example is assuming that standardized coefficients usually fall between -3 and 3, leading to a specification like `pm.Normal('coef', mu=0, sigma=1)`. This approach prevents extreme parameter values while remaining relatively non-committal about the exact values.\n",
    "\n",
    "**Non-informative priors** are designed to let the data dominate the analysis by expressing minimal prior knowledge. These priors attempt to be \"objective\" by spreading probability mass widely across the parameter space. An example might be `pm.Uniform('param', lower=-100, upper=100)` for a parameter where we have little prior knowledge. However, truly non-informative priors are often difficult to specify and can sometimes lead to computational problems.\n",
    "\n",
    "### Prior Choice Guidelines\n",
    "\n",
    "Effective prior specification requires balancing several considerations to ensure both computational stability and meaningful results. **Starting with weakly informative priors** provides an excellent default approach because they offer numerical stability without imposing strong constraints on the analysis. These priors help prevent the sampler from exploring extreme regions of the parameter space that might be computationally problematic.\n",
    "\n",
    "**Using domain knowledge** when available can significantly improve model performance and interpretability. Subject matter expertise often provides valuable constraints on reasonable parameter ranges, and incorporating this knowledge through informative priors can lead to more realistic and interpretable results.\n",
    "\n",
    "**Checking prior predictive distributions** serves as a crucial validation step in the modeling process. Before observing any data, we should simulate from our prior distributions to ensure they generate reasonable predictions. If our prior predictive distributions produce implausible results, this indicates that our prior specifications need adjustment.\n",
    "\n",
    "Finally, **testing sensitivity** to prior choices helps assess the robustness of our conclusions. By fitting the model with different reasonable prior specifications and comparing the results, we can determine how much our conclusions depend on prior assumptions versus the observed data. Substantial sensitivity to prior choice may indicate that we need more data or more carefully considered priors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood Choice: Matching Model to Data\n",
    "\n",
    "The **likelihood function** connects parameters to data by specifying the probability distribution of observations given parameter values.\n",
    "\n",
    "### Common Likelihood Functions\n",
    "\n",
    "**Normal** for continuous, symmetric data: `pm.Normal('obs', mu=mu, sigma=sigma)`  \n",
    "**Poisson** for count data: `pm.Poisson('counts', mu=rate)`  \n",
    "**Binomial** for binary outcomes: `pm.Binomial('successes', n=n_trials, p=prob)`\n",
    "\n",
    "### Selection Guidelines\n",
    "\n",
    "**Match data characteristics**: Consider whether your data are continuous, counts, or proportions. Check key assumptions like symmetry (Normal) or mean-variance relationship (Poisson).\n",
    "\n",
    "**Use robust alternatives when needed**: Student-t handles outliers better than Normal. Negative Binomial accommodates overdispersed counts better than Poisson.\n",
    "\n",
    "**Let the data-generating process guide you**: Time between events suggests Exponential; measurement errors often follow Normal; rare events suit Poisson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import warnings\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RNG = np.random.default_rng(RANDOM_SEED:=42)\n",
    "\n",
    "print(\"🔧 Libraries loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, let's build a very simple model using the births dataset from the last section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "births_data = pl.read_csv('../data/births.csv', null_values=['null', 'NA', '', 'NULL'])\n",
    "births_data = births_data.filter(pl.col('day').is_not_null())\n",
    "\n",
    "# Aggregate to monthly data\n",
    "monthly_births = (births_data\n",
    "    .group_by(['year', 'month'])\n",
    "    .agg(pl.col('births').sum())\n",
    "    .sort(['year', 'month'])\n",
    ")\n",
    "\n",
    "# Focus on 1970-1990 period\n",
    "births_subset = (monthly_births\n",
    "    .filter((pl.col('year') >= 1970) & (pl.col('year') <= 1990))\n",
    "    .with_row_index('index')\n",
    ")\n",
    "\n",
    "# Standardize the data\n",
    "original_data = births_subset['births'].to_numpy()\n",
    "births_standardized = (original_data - original_data.mean()) / original_data.std()\n",
    "\n",
    "print(f\"📊 Data loaded: {len(births_standardized)} observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyMC API and Workflow\n",
    "\n",
    "PyMC provides a high-level interface for building Bayesian models. The typical workflow involves:\n",
    "\n",
    "1. **Model Definition**: Specify priors, likelihood, and relationships\n",
    "2. **Model Fitting**: Use MCMC or another method to approximate the posterior\n",
    "3. **Diagnostics**: Check convergence and model fit\n",
    "4. **Analysis**: Summarize results and make predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Regression Model\n",
    "\n",
    "Now let's implement a more sophisticated model that can capture trends in the data. Instead of assuming a constant mean, we'll use a **polynomial regression** model that can capture linear and quadratic trends over time.\n",
    "\n",
    "### Model Specification\n",
    "\n",
    "Our polynomial regression model is:\n",
    "\n",
    "$$y_t = \\mu + \\beta_1 \\cdot t + \\beta_2 \\cdot t^2 + \\epsilon_t$$\n",
    "\n",
    "where:\n",
    "- $\\mu$ is the intercept\n",
    "- $\\beta_1$ is the linear trend coefficient\n",
    "- $\\beta_2$ is the quadratic trend coefficient\n",
    "- $\\epsilon_t \\sim \\mathcal{N}(0, \\sigma^2)$ is the observation noise\n",
    "\n",
    "This model allows us to capture both linear trends and curvature in the time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with pm.Model(coords={'time': time_idx, 'coef': ['linear', 'quadratic']}) as poly_model:\n",
    "\n",
    "    mu = pm.Normal('mu', mu=0, sigma=1)\n",
    "    # Vector-valued polynomial regression coefficients with informative priors\n",
    "    beta = pm.Normal('beta', mu=0, sigma=1, dims='coef')\n",
    "    \n",
    "    X = np.column_stack([\n",
    "        time_normalized,\n",
    "        time_normalized**2\n",
    "    ])\n",
    "    \n",
    "    # Expected value (quadratic polynomial)\n",
    "    mu_t = pm.Deterministic('mu_t', mu + pm.math.dot(X, beta), dims='time')\n",
    "    \n",
    "    # Observation noise\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    \n",
    "    # Likelihood\n",
    "    y_obs = pm.Normal('y_obs', mu=mu_t, sigma=sigma, observed=births_standardized, dims='time')\n",
    "    \n",
    "    # Sample from the posterior\n",
    "    trace_poly = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, chains=4)\n",
    "\n",
    "print(az.summary(trace_poly, var_names=['beta', 'sigma']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(poly_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_post = trace_poly.posterior['mu'].values.flatten()\n",
    "beta_1_post = trace_poly.posterior['beta'].sel(coef='linear').values.flatten()\n",
    "beta_2_post = trace_poly.posterior['beta'].sel(coef='quadratic').values.flatten()\n",
    "mu_t_post = trace_poly.posterior['mu_t'].values\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "mu_t_reshaped = mu_t_post.reshape(-1, mu_t_post.shape[-1])\n",
    "\n",
    "mu_mean = np.mean(mu_t_reshaped, axis=0)\n",
    "mu_lower = np.percentile(mu_t_reshaped, 2.5, axis=0)\n",
    "mu_upper = np.percentile(mu_t_reshaped, 97.5, axis=0)\n",
    "\n",
    "ax1.fill_between(time_idx, mu_lower, mu_upper, alpha=0.3, color='blue', label='95% CI')\n",
    "ax1.plot(time_idx, mu_mean, color='blue', linewidth=2, label='Posterior Mean')\n",
    "ax1.scatter(time_idx, births_standardized, alpha=0.6, color='red', s=20, label='Observed Data')\n",
    "ax1.set_title('Polynomial Regression Fit')\n",
    "ax1.set_xlabel('Time Index')\n",
    "ax1.set_ylabel('Standardized Births')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.hist(mu_post, bins=50, alpha=0.7, density=True, label='μ (intercept)')\n",
    "ax2.axvline(np.mean(mu_post), color='red', linestyle='--', label=f'Mean: {np.mean(mu_post):.3f}')\n",
    "ax2.set_title('Posterior: Intercept (μ)')\n",
    "ax2.set_xlabel('Value')\n",
    "ax2.set_ylabel('Density')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "ax3.hist(beta_1_post, bins=50, alpha=0.7, density=True, label='β₁ (linear)', color='green')\n",
    "ax3.axvline(np.mean(beta_1_post), color='red', linestyle='--', label=f'Mean: {np.mean(beta_1_post):.3f}')\n",
    "ax3.set_title('Posterior: Linear Term (β₁)')\n",
    "ax3.set_xlabel('Value')\n",
    "ax3.set_ylabel('Density')\n",
    "ax3.legend()\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "ax4.hist(beta_2_post, bins=50, alpha=0.7, density=True, label='β₂ (quadratic)', color='orange')\n",
    "ax4.axvline(np.mean(beta_2_post), color='red', linestyle='--', label=f'Mean: {np.mean(beta_2_post):.3f}')\n",
    "ax4.set_title('Posterior: Quadratic Term (β₂)')\n",
    "ax4.set_xlabel('Value')\n",
    "ax4.set_ylabel('Density')\n",
    "ax4.legend()\n",
    "ax4.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Diagnostics: Ensuring Reliable Inference\n",
    "\n",
    "Before trusting our results, we must check that our MCMC sampling worked properly. Poor sampling can lead to incorrect conclusions.\n",
    "\n",
    "### Key Diagnostic Metrics\n",
    "\n",
    "1. **R-hat (Gelman-Rubin statistic)**: Measures convergence across chains\n",
    "   - **Good**: R-hat ≤ 1.01\n",
    "   - **Acceptable**: R-hat ≤ 1.1  \n",
    "   - **Poor**: R-hat > 1.1\n",
    "\n",
    "2. **Effective Sample Size (ESS)**: Number of independent samples\n",
    "   - **Good**: ESS > 400 (for tail quantities)\n",
    "   - **Minimum**: ESS > 100\n",
    "\n",
    "3. **Energy diagnostics**: Check for sampling pathologies\n",
    "   - **E-BFMI**: Energy Bayesian Fraction of Missing Information\n",
    "   - **Divergences**: Indicate problematic posterior geometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 **Model Diagnostics and Convergence Checking**\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Check basic diagnostics\n",
    "print(\"\\n📊 **Summary Statistics with Diagnostics**:\")\n",
    "summary = az.summary(trace_poly, var_names=['beta', 'sigma'])\n",
    "print(summary)\n",
    "\n",
    "# Plot trace plots for visual inspection\n",
    "print(\"\\n📈 **Trace Plots** (should look like 'fuzzy caterpillars'):\")\n",
    "az.plot_trace(trace_poly, var_names=['beta', 'sigma'], figsize=(12, 6))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check for divergences and other warnings\n",
    "print(\"\\n⚠️  **Sampling Diagnostics**:\")\n",
    "print(f\"   • Number of divergences: {trace_poly.sample_stats.diverging.sum().values}\")\n",
    "print(f\"   • Max tree depth reached: {(trace_poly.sample_stats.tree_depth >= 10).sum().values} times\")\n",
    "\n",
    "# Energy plot for convergence checking\n",
    "print(\"\\n⚡ **Energy Diagnostics**:\")\n",
    "az.plot_energy(trace_poly, figsize=(10, 4))\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n✅ **Diagnostic Interpretation**:\")\n",
    "print(\"   • R-hat close to 1.0 → Good convergence\")\n",
    "print(\"   • High ESS → Many effective samples\")\n",
    "print(\"   • No divergences → Sampling was stable\")\n",
    "print(\"   • Energy plots overlap → No pathological behavior\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having fit our model and ensured that the MCMC sampler has converged, we can now use it as a **generative model** to simulate datasets. We can use thes predictive samples to check the validity of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior predictive checks\n",
    "with poly_model:\n",
    "    posterior_predictive = pm.sample_posterior_predictive(trace_poly, random_seed=RANDOM_SEED)\n",
    "\n",
    "# Plot comparison using ArviZ\n",
    "az.plot_ppc(posterior_predictive, num_pp_samples=50, figsize=(10, 6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Complete Bayesian Workflow\n",
    "\n",
    "Let's summarize the essential steps of Bayesian analysis that we've demonstrated:\n",
    "\n",
    "### 1. Model Specification\n",
    "```python\n",
    "with pm.Model() as model:\n",
    "    # Priors\n",
    "    theta = pm.Normal('theta', mu=0, sigma=1)\n",
    "    # Likelihood  \n",
    "    obs = pm.Normal('obs', mu=theta, sigma=1, observed=data)\n",
    "```\n",
    "\n",
    "### 2. Prior Predictive Checking\n",
    "```python\n",
    "prior_pred = pm.sample_prior_predictive(model)\n",
    "# Check: Do simulated data look reasonable? If not, go back and revise model.\n",
    "```\n",
    "\n",
    "### 3. Posterior Sampling\n",
    "```python\n",
    "trace = pm.sample(model)\n",
    "```\n",
    "\n",
    "### 4. Convergence Diagnostics\n",
    "```python\n",
    "az.summary(trace)  \n",
    "az.plot_trace(trace)  \n",
    "# If we are doing MCMC, ensure that the sampler has converged\n",
    "```\n",
    "\n",
    "### 5. Posterior Predictive Checking\n",
    "```python\n",
    "post_pred = pm.sample_posterior_predictive(trace, model)\n",
    "az.plot_ppc(post_pred)  \n",
    "# Check: Do the results make sense? Is there reasonable model fit? If not, revise model.\n",
    "```\n",
    "\n",
    "### 6. Inference and Decision Making\n",
    "```python\n",
    "# Probability statements\n",
    "prob_positive = (samples > 0).mean()\n",
    "# Credible intervals\n",
    "hdi = az.hdi(samples, hdi_prob=0.95)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "Gelman, Andrew, John B. Carlin, Hal S. Stern, David B. Dunson, Aki Vehtari, and Donald B. Rubin. 2013. Bayesian Data Analysis, Third Edition. CRC Press.\n",
    "\n",
    "Downey, Allen. 2021. Think Bayes: Bayesian Statistics in Python, Second Edition. O'Reilly Media.\n",
    "\n",
    "Pilon, Cam-Davidson. Probabilistic Programming and Bayesian Methods for Hackers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark -n -u -v -iv -w"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
