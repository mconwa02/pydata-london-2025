{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zeJ1uUTuDd40"
   },
   "source": [
    "# Demo Frozen Lake Q-Learning Agent\n",
    " - Demos the Q-Learning off-policy RL agent for the Frozen Lake problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_8PddWowDxcv"
   },
   "source": [
    "#### Installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11862,
     "status": "ok",
     "timestamp": 1748685083051,
     "user": {
      "displayName": "adeidowu@hotmail.com",
      "userId": "15313548625752705445"
     },
     "user_tz": -60
    },
    "id": "y6Y6a5yNDQDn",
    "outputId": "bf414604-441d-41a3-9090-0ee4c32652b7"
   },
   "outputs": [],
   "source": [
    "# ! pip install gymnasium numpy matplotlib pyvirtualdisplay\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aOk_BGXND8oO"
   },
   "source": [
    "#### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1748685083820,
     "user": {
      "displayName": "adeidowu@hotmail.com",
      "userId": "15313548625752705445"
     },
     "user_tz": -60
    },
    "id": "3izkMzktEFjw"
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from dataclasses import asdict\n",
    "\n",
    "sys.path.insert(1, \"../\")\n",
    "\n",
    "from src.main.rl_agents.sarsa_agent import SarsaAgent\n",
    "from src.main.configs.q_learning_agent_configs import QLearningAgentConfig\n",
    "import src.main.configs.global_configs as configs\n",
    "from src.main.utility.utils import Helpers\n",
    "from src.main.utility.chart_results import ChartResults\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vto65rpOz4w3"
   },
   "source": [
    "#### Define global configs and variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1748685083827,
     "user": {
      "displayName": "adeidowu@hotmail.com",
      "userId": "15313548625752705445"
     },
     "user_tz": -60
    },
    "id": "BZTJ2zRIz36Z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-FLRSj54zNs"
   },
   "source": [
    "#### Utility class of helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1748685083837,
     "user": {
      "displayName": "adeidowu@hotmail.com",
      "userId": "15313548625752705445"
     },
     "user_tz": -60
    },
    "id": "JyhC9PT445_F"
   },
   "outputs": [],
   "source": [
    "class Helpers:\n",
    "  \"\"\"\n",
    "  Utility class of helper functions\n",
    "  \"\"\"\n",
    "  @staticmethod\n",
    "  def animateEnvironment(images: List[Any]):\n",
    "    \"\"\"\n",
    "    Animates the environment\n",
    "    :param images: Images\n",
    "    \"\"\"\n",
    "    plt.figure(\n",
    "        figsize=(images[0].shape[1]/DPI,images[0].shape[0]/DPI),\n",
    "        dpi=DPI\n",
    "        )\n",
    "    patch = plt.imshow(images[0])\n",
    "    plt.axis=('off')\n",
    "    animate = lambda i: patch.set_data(images[i])\n",
    "    ani = FuncAnimation(\n",
    "        plt.gcf(),\n",
    "        animate,\n",
    "        frames=len(images),\n",
    "        interval=INTERVAL)\n",
    "    display.display(display.HTML(ani.to_jshtml()))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M8bAHKyxz21l"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsdKTSxMGCiw"
   },
   "source": [
    "#### Solution 3 Steps:\n",
    " - Step 1: Demonstrate the performance of a random agent\n",
    " - Step 2: Implement Q-learning RL (off-policy) agent\n",
    " - Step 3: Implement the RL training loop\n",
    " - Step 4: Implement the RL evaluation (animation) policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 1: Demonstrate the performance of a random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVdGXea-EUes"
   },
   "source": [
    "##### Step 1: Q-learning RL (off-policy) implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 40,
     "status": "ok",
     "timestamp": 1748685083880,
     "user": {
      "displayName": "adeidowu@hotmail.com",
      "userId": "15313548625752705445"
     },
     "user_tz": -60
    },
    "id": "4Wc3PmEDEkU1"
   },
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    def __init__(\n",
    "        self,\n",
    "        env,\n",
    "        alpha=0.1,\n",
    "        gamma=0.99,\n",
    "        epsilon=1.0,\n",
    "        epsilon_decay=0.995,\n",
    "        min_epsilon=0.01\n",
    "        ):\n",
    "        self.env = env\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.min_epsilon = min_epsilon\n",
    "        n_states = env.observation_space.n\n",
    "        n_actions = env.action_space.n\n",
    "        self.Q = np.zeros((n_states, n_actions)) # + np.random.rand(n_states, n_actions)\n",
    "\n",
    "        print(f\"Frozen Lake environment creation..\")\n",
    "        print(f\"Observation space: {n_states}\")\n",
    "        print(f\"Action space: {n_actions}\")\n",
    "        print(f\"\"\"Q-learning hyperparameters are:\n",
    "                  \\nalpha: {self.alpha}\n",
    "                  \\nepsilon: {self.epsilon}\n",
    "                  \\nepsilon_decay: {self.epsilon_decay}\n",
    "                  \\nmin_epsilon: {self.min_epsilon}\\n\"\"\")\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        return np.argmax(self.Q[state])\n",
    "\n",
    "    def update(self, s, a, r, s_next):\n",
    "        td_target = r + self.gamma * np.max(self.Q[s_next, :])\n",
    "        td_error = td_target - self.Q[s, a]\n",
    "        self.Q[s, a] += self.alpha * td_error\n",
    "\n",
    "    def decay_epsilon(self):\n",
    "        self.epsilon = max(self.min_epsilon, self.epsilon * self.epsilon_decay)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OWxue1zoFUZS"
   },
   "source": [
    "##### Step 2: Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 14383,
     "status": "ok",
     "timestamp": 1748685098265,
     "user": {
      "displayName": "adeidowu@hotmail.com",
      "userId": "15313548625752705445"
     },
     "user_tz": -60
    },
    "id": "v_6t-DY7FWh2",
    "outputId": "43fdf09d-a44a-4f86-d388-e5bcca9610ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen Lake environment creation..\n",
      "Observation space: 16\n",
      "Action space: 4\n",
      "Q-learning hyperparameters are:\n",
      "                  \n",
      "alpha: 0.1\n",
      "                  \n",
      "epsilon: 1.0\n",
      "                  \n",
      "epsilon_decay: 0.995\n",
      "                  \n",
      "min_epsilon: 0.01\n",
      "\n",
      "Episode 500/5000  Average Reward: 0.052\n",
      "\n",
      "Q: [[5.04735017e-02 7.59574376e-02 5.81532423e-02 4.22135452e-02]\n",
      " [3.12405076e-02 5.01389442e-02 4.32351330e-02 9.13053396e-02]\n",
      " [1.12245806e-01 4.81018750e-02 3.77343726e-02 2.77865895e-02]\n",
      " [2.98164677e-05 1.27169496e-02 1.02458477e-03 3.26553452e-02]\n",
      " [7.33865790e-02 3.58944637e-02 4.99878207e-02 3.57057765e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.53949025e-01 3.33126510e-02 3.26730677e-02 6.21413701e-03]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [2.10226676e-02 2.62173539e-02 6.81245527e-02 5.01887733e-02]\n",
      " [1.48494878e-02 7.23158292e-02 1.74374955e-01 3.82786225e-02]\n",
      " [2.38519910e-01 7.14120665e-02 9.11908948e-02 1.19831756e-02]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [3.68293641e-02 8.01967038e-02 2.65758557e-01 7.48478786e-02]\n",
      " [4.97170565e-02 2.10798598e-01 1.27924697e-01 4.00307419e-01]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00]]\n",
      "\n",
      "Episode 1000/5000  Average Reward: 0.280\n",
      "\n",
      "Q: [[0.26070975 0.34800114 0.25048905 0.21948211]\n",
      " [0.12391263 0.0870927  0.1300034  0.3073998 ]\n",
      " [0.1949837  0.19455614 0.16327607 0.28703078]\n",
      " [0.07557984 0.13808651 0.07376796 0.27853904]\n",
      " [0.37980851 0.1105611  0.24839063 0.08936378]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.23496646 0.0323754  0.03267307 0.0295756 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05896603 0.21855738 0.12262084 0.42194353]\n",
      " [0.09860297 0.48993406 0.16638538 0.08575274]\n",
      " [0.49158664 0.16487602 0.16493309 0.05162952]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.15535426 0.2553032  0.57005997 0.14266766]\n",
      " [0.09094844 0.38734038 0.31714573 0.71052355]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Episode 1500/5000  Average Reward: 0.518\n",
      "\n",
      "Q: [[0.4574169  0.37446617 0.37318762 0.34763548]\n",
      " [0.17171009 0.19969162 0.11338852 0.42915717]\n",
      " [0.21538827 0.22377401 0.19207009 0.36323403]\n",
      " [0.17935966 0.17727639 0.1431575  0.34820704]\n",
      " [0.46959535 0.26033081 0.25900573 0.2063169 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.16922951 0.0323754  0.02940576 0.02661804]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.16245099 0.23852264 0.32115353 0.48467353]\n",
      " [0.19924346 0.49909345 0.22907529 0.12493687]\n",
      " [0.5438852  0.30835751 0.14843978 0.06671881]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.30717183 0.26918933 0.56499751 0.23504289]\n",
      " [0.27877402 0.47638951 0.31714573 0.79059914]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Episode 2000/5000  Average Reward: 0.542\n",
      "\n",
      "Q: [[0.428075   0.40933619 0.41831167 0.39680777]\n",
      " [0.20366971 0.24400538 0.12851735 0.40996414]\n",
      " [0.2448745  0.26038947 0.21787072 0.37725167]\n",
      " [0.21149975 0.17738451 0.18436967 0.36315272]\n",
      " [0.44199824 0.35106258 0.25562212 0.28824808]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.35289345 0.0323754  0.06035145 0.02395623]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.34435369 0.29210846 0.30974662 0.47150881]\n",
      " [0.20465672 0.50020684 0.26484338 0.14631534]\n",
      " [0.44155929 0.30835751 0.15584812 0.14270981]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.26639225 0.36072024 0.58046808 0.25958713]\n",
      " [0.36023054 0.53596273 0.41590752 0.63494338]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Episode 2500/5000  Average Reward: 0.584\n",
      "\n",
      "Q: [[0.55357124 0.4088162  0.39549252 0.42278649]\n",
      " [0.20366971 0.20690905 0.17126897 0.45860505]\n",
      " [0.26120658 0.2826321  0.24768395 0.41476674]\n",
      " [0.20599213 0.17738451 0.17848142 0.39110739]\n",
      " [0.56929637 0.33108243 0.28927282 0.42478663]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.24102634 0.02622408 0.08691849 0.02395623]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.25800375 0.32545949 0.3393109  0.60462939]\n",
      " [0.3964809  0.64256776 0.23088156 0.21103021]\n",
      " [0.57975385 0.27752176 0.23521058 0.14270981]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.2976989  0.3757078  0.75047311 0.29666902]\n",
      " [0.36023054 0.8392028  0.53993276 0.67429445]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Episode 3000/5000  Average Reward: 0.704\n",
      "\n",
      "Q: [[0.58282452 0.49398118 0.49839333 0.51185966]\n",
      " [0.20366971 0.18621815 0.15414207 0.50042775]\n",
      " [0.31636804 0.29469919 0.25968659 0.46653218]\n",
      " [0.22967579 0.22606756 0.14439314 0.43858161]\n",
      " [0.60888655 0.4307209  0.41808263 0.31690984]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.28184408 0.02622408 0.1227499  0.13611214]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.22662037 0.34349739 0.43877354 0.62598664]\n",
      " [0.39964038 0.64518157 0.46979517 0.3257888 ]\n",
      " [0.56852656 0.28286277 0.23521058 0.14270981]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.41443869 0.38721908 0.77374487 0.40647463]\n",
      " [0.44050212 0.88602725 0.58593948 0.69446617]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Episode 3500/5000  Average Reward: 0.694\n",
      "\n",
      "Q: [[0.56634479 0.5191312  0.5155218  0.50721797]\n",
      " [0.20366971 0.21294188 0.15414207 0.50760911]\n",
      " [0.33366938 0.31290276 0.2788284  0.47392414]\n",
      " [0.25118953 0.20346081 0.17344558 0.45197919]\n",
      " [0.57706867 0.26375207 0.37845707 0.3644483 ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.33092521 0.0803829  0.11047491 0.13611214]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.32324026 0.37716537 0.36665583 0.62010967]\n",
      " [0.42905542 0.67758954 0.47845107 0.36324014]\n",
      " [0.77537458 0.32333976 0.2803898  0.12843883]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.37299482 0.41448907 0.7316185  0.4264084 ]\n",
      " [0.50621272 0.90973879 0.66461098 0.76171216]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Episode 4000/5000  Average Reward: 0.688\n",
      "\n",
      "Q: [[0.54249831 0.49630923 0.50599141 0.5079254 ]\n",
      " [0.20366971 0.21958103 0.16643234 0.50371596]\n",
      " [0.33366938 0.3137897  0.2788284  0.47482202]\n",
      " [0.26879004 0.24986208 0.22463387 0.44937382]\n",
      " [0.56393953 0.3955689  0.37488064 0.35367419]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.40515909 0.07234461 0.13557925 0.12250093]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.31524822 0.46694638 0.39675053 0.60419015]\n",
      " [0.41020277 0.68247993 0.51019569 0.45648711]\n",
      " [0.66532468 0.31596357 0.30359129 0.10403545]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.48267389 0.40668253 0.79740762 0.42658594]\n",
      " [0.50621272 0.89608822 0.68529927 0.77065112]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Episode 4500/5000  Average Reward: 0.670\n",
      "\n",
      "Q: [[0.5867335  0.49423104 0.4947391  0.50928282]\n",
      " [0.20366971 0.21958103 0.16643234 0.4967338 ]\n",
      " [0.3709484  0.30999844 0.3155962  0.45990195]\n",
      " [0.26207057 0.2856962  0.22463387 0.44790781]\n",
      " [0.60518476 0.44630344 0.33953998 0.33644873]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.32930839 0.06511015 0.17481803 0.11025083]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.3481226  0.33973813 0.41006975 0.64045938]\n",
      " [0.44930886 0.7091079  0.41325851 0.49516098]\n",
      " [0.62253593 0.32878887 0.32493081 0.14127246]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.31668234 0.36601428 0.80132395 0.45168809]\n",
      " [0.53209786 0.88923947 0.67965166 0.78512733]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n",
      "Episode 5000/5000  Average Reward: 0.658\n",
      "\n",
      "Q: [[0.5209605  0.48406961 0.48192598 0.48831196]\n",
      " [0.18330274 0.2509574  0.1999725  0.49166624]\n",
      " [0.40205433 0.3310075  0.3155962  0.46077834]\n",
      " [0.25186688 0.31752526 0.24627613 0.43659177]\n",
      " [0.54871914 0.28859551 0.40938563 0.34838054]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.21822059 0.06511015 0.33210928 0.11025083]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.43085154 0.32440136 0.4254001  0.58887803]\n",
      " [0.50837478 0.66557869 0.38838731 0.43759855]\n",
      " [0.69986596 0.32878887 0.26062719 0.14552279]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.32435184 0.39568604 0.78388642 0.40651928]\n",
      " [0.53209786 0.90516306 0.69635396 0.78733033]\n",
      " [0.         0.         0.         0.        ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(agent, env, n_episodes=5000, max_steps=100):\n",
    "    rewards = []\n",
    "    for ep in range(n_episodes):\n",
    "        s, _ = env.reset()                      # start new episode\n",
    "\n",
    "        # print(f\"s: {s}\\ta: {a}\")\n",
    "        total_reward = 0\n",
    "\n",
    "        for _ in range(max_steps):\n",
    "            a = agent.choose_action(s)\n",
    "            s_next, r, done, _, _ = env.step(a)\n",
    "            # a_next = np.argmax(agent.Q[s_next])\n",
    "            # print(f\"s: {s}, a: {a}, r: {r}, s_next: {s_next}\")\n",
    "            agent.update(s, a, r, s_next)\n",
    "            # s, a = s_next, a_next\n",
    "            s = s_next\n",
    "\n",
    "            total_reward += r\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        agent.decay_epsilon()\n",
    "        rewards.append(total_reward)\n",
    "        if (ep+1) % 500 == 0:\n",
    "            print(f\"Episode {ep+1}/{n_episodes}  Average Reward: {np.mean(rewards[-500:]):.3f}\")\n",
    "            print(f\"\\nQ: {agent.Q}\\n\")\n",
    "    return rewards\n",
    "\n",
    "# Create environment\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode=\"rgb_array\")\n",
    "agent = QLearningAgent(env)\n",
    "\n",
    "# Train\n",
    "training_rewards = train(agent, env)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9Si9GGJFb-R"
   },
   "source": [
    "##### Step 3: Implement the RL evaluation (animation) policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1748685098270,
     "user": {
      "displayName": "adeidowu@hotmail.com",
      "userId": "15313548625752705445"
     },
     "user_tz": -60
    },
    "id": "8mQ6vPYVF1QG"
   },
   "outputs": [],
   "source": [
    "def animate_policy(agent, env, fps=2):\n",
    "    # Start virtual display\n",
    "    display = Display(visible=0, size=(400, 400))\n",
    "    display.start()\n",
    "\n",
    "    n_rows, n_cols = agent.Q.shape\n",
    "    states, actions = [], []\n",
    "    s, _ = env.reset()\n",
    "    states.append(s)\n",
    "\n",
    "    # Rollout under greedy policy\n",
    "    for _ in range(100):\n",
    "        a = np.argmax(agent.Q[s])\n",
    "        s, _, done, _, _ = env.step(a)\n",
    "        states.append(s)\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    # Setup plot\n",
    "    fig, ax = plt.subplots()\n",
    "    # ax.set_xlim(0, env.desc.shape[1])\n",
    "    # ax.set_ylim(0, env.desc.shape[0])\n",
    "    ax.set_xlim(0, n_cols)\n",
    "    ax.set_ylim(0, n_rows)\n",
    "    agent_dot, = ax.plot([], [], 'ro', ms=20)\n",
    "\n",
    "    def init():\n",
    "        agent_dot.set_data([], [])\n",
    "        return agent_dot,\n",
    "\n",
    "    def update(frame):\n",
    "        # Convert state index to (row, col)\n",
    "        row, col = divmod(states[frame], n_cols)\n",
    "        agent_dot.set_data(col + 0.5, n_rows - row - 0.5)\n",
    "        return agent_dot,\n",
    "\n",
    "    anim = FuncAnimation(fig, update, init_func=init,\n",
    "                         frames=len(states), interval=1000/fps, repeat=False)\n",
    "\n",
    "\n",
    "    plt.close(fig)  # prevent static display\n",
    "    display.stop()\n",
    "    return anim\n",
    "\n",
    "# Generate and display the animation in, e.g., a Jupyter notebook\n",
    "# anim = animate_policy(agent, env)\n",
    "# display.display(HTML(anim.to_jshtml()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1748685098284,
     "user": {
      "displayName": "adeidowu@hotmail.com",
      "userId": "15313548625752705445"
     },
     "user_tz": -60
    },
    "id": "li5r20Kzzm1x"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1748685098292,
     "user": {
      "displayName": "adeidowu@hotmail.com",
      "userId": "15313548625752705445"
     },
     "user_tz": -60
    },
    "id": "aadGCl3DzmnX"
   },
   "outputs": [],
   "source": [
    "class EvaluateAgent:\n",
    "  \"\"\"\n",
    "  Evaluate the Q learning RL agent using animation of the simulation runs\n",
    "  \"\"\"\n",
    "  def __init__(self, agent, env, n_episodes=2, max_steps=100):\n",
    "    \"\"\"\n",
    "    Constructor\n",
    "    \"\"\"\n",
    "    self.agent = agent\n",
    "    self.env = env\n",
    "    self.n_episodes = n_episodes\n",
    "    self.max_steps = max_steps\n",
    "    # self.display = Display(visible=0, size=(400, 400))\n",
    "    # self.display.start()\n",
    "    self.states = []\n",
    "\n",
    "    self.images = []\n",
    "\n",
    "  def _evaluate(self):\n",
    "    \"\"\"\n",
    "    Evaluate the agent\n",
    "    \"\"\"\n",
    "    for ep in range(self.n_episodes):\n",
    "      s, _ = self.env.reset()\n",
    "      self.states.append(s)\n",
    "\n",
    "      # Rollout under greedy policy\n",
    "      for _ in range(self.max_steps):\n",
    "          a = np.argmax(agent.Q[s])\n",
    "          s, _, done, _, _ = self.env.step(a)\n",
    "          self.env.render()\n",
    "          #self.images.append(self.env.render())\n",
    "          self.states.append(s)\n",
    "          if done:\n",
    "              break\n",
    "\n",
    "      # self.env.close()\n",
    "\n",
    "\n",
    "  def run(self):\n",
    "    \"\"\"\n",
    "    Run the RL evaluation with animation\n",
    "    \"\"\"\n",
    "    self._evaluate()\n",
    "    # Helpers.animateEnvironment(self.images)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "executionInfo": {
     "elapsed": 32455,
     "status": "ok",
     "timestamp": 1748685130746,
     "user": {
      "displayName": "adeidowu@hotmail.com",
      "userId": "15313548625752705445"
     },
     "user_tz": -60
    },
    "id": "YyDlwU487-sf",
    "outputId": "4cd46862-8796-4ca5-a7b2-299f70ca957c"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode=\"human\")\n",
    "evaluate = EvaluateAgent(agent, env)\n",
    "evaluate.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IscCvPwAF5J_"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPJq1FNbSsNM03d6r/mzeYA",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (3.11) rl_env",
   "language": "python",
   "name": "rl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
